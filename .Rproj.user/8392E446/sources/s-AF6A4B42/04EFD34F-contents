---
title: "NFL Losers Pool 2021 Optimal Pick Algorithm"
author: "Jordan Hutchings"
date: "08/11/2021"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    code_folding: hide
---

```{r, echo = FALSE}
# setup ----
pacman::p_load(dplyr, ggplot2, kableExtra, data.table, combinat, scales)
```

## The 2021 NFL Losers Pool

I've entered into an annual NFL Losers Pool with some friends. A Losers Pool is a 
fantasy NFL league where each competitor in the league is required to pick any team 
playing that week to lose their game. If the picked team wins, the player is eliminated 
from the pool. The players who correctly picked a losing team that week move on and 
must pick a new team to lose the following week. The last person remaining in 
the Losers Pool is crowned the champion. 

Arguably, I may be the least informed competitor in the pool. Especially since 
I haven't followed football since I lost out of the 2020 Losers Pool, however, 
I have a trick up my sleeve and plan on creating an optimization algorithm to 
outperform the others in the pool. 

My strategy is to use game-by-game forecasts for the 2021 season, then optimize 
my picks maximizing the likelihood of reaching a given week. Hopefully, with 
enough foresight, data analytics, and some luck, I'll be able to outlast my 
competitors. 

While it would have been a great exercise to forecast the NFL games, 
I've instead choose to leverage the NFL game projections made by 
[FiveThirtyEight](https://projects.fivethirtyeight.com/2021-nfl-predictions/games/). 
These projections are based off their quarterback-adjusted ELO forecasts. Lets 
quickly review ELO ratings for those who are unfamiliar, or just need a touch-up 
on how we can leverage the probability of winning from the ratings. 

## ELO Ratings

ELO is a fantastic and widely used method of ranking two competing teams. 
Commonly used in chess, your ELO score is a representation of your skill based 
on your performance against other ELO ranked players. There is a great video 
describing the ELO process made by James Grime on his 
[youtube channel](https://www.youtube.com/watch?v=AsYfbmp0To0&ab_channel=singingbanana). 
All we really need to know about ELO is that we can derive the probability 
a given team A beats the other team B as: 

$$Pr(\text{A Wins}) = \frac{1}{1+10^{(ELO_B -ELO_A)/400}} $$
The FiveThirtyEight team has compiled the ELO rankings over time for each NFL 
team based on their past results, and uses these scores - with some modifications 
such as, home advantage, and skill of the quarterback - to forecast the average 
win probabilities for each game of the season. 

### ELO Rankings throughout the 2021 season

We can get a sense of the ELO rankings throughout a season by looking at the 
historical ELO rankings for last year's 2020 season. 

```{r, message=FALSE}
elo = read.csv("https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv") %>%
  filter(season %in% c("2020")) %>%
  select(date, team1, team2, elo1_pre, elo2_pre) %>%
  tidyr::pivot_longer(cols = c("team1", "team2"), names_to = c("team_order"), values_to = ("team")) %>%
  mutate(elo = case_when(
    team_order == "team1" ~ elo1_pre, 
    team_order == "team2" ~ elo2_pre), 
    date = as.Date(date)) %>%
  select(date, team, elo) %>%
  mutate(team = reorder(team, elo)) %>%
  filter(date < as.Date("01-09-2021", "%m-%d-%Y"))

ggplot(elo, aes(x = date, y = elo, color = team)) + 
  geom_line() + 
  scale_color_viridis_d() + 
  labs(title = "ELO rankings through the 2020 regular season", 
       x = "", 
       y = "ELO Ranking") + 
  theme_bw(12) + 
  theme(legend.position="none")
```

### Historical ELO performance

We want to make sure ELO rankings are creditable to use for our game predictions. 
If the probabilities of winning each game derived from ELO are creditable, then 
we would expect games with a 80% probability to be won 80% of the time. We can 
verify this using the historical ELO data dating back to 1950.
 
If the ELO rankings are a good indicator of the game outcome, we can expect to 
see a 45 degree line between ELO projected game outcomes and the actual 
observed outcomes. 

```{r}
data = read.csv("https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv") %>%
  mutate(loser_score = ifelse(qbelo_prob1 > qbelo_prob2, score2, score1), 
         winner_score = ifelse(qbelo_prob1 > qbelo_prob2, score1, score2), 
         p_win_winner = ifelse(qbelo_prob1 > qbelo_prob2, qbelo_prob1, qbelo_prob2), 
         win = as.numeric(winner_score > loser_score), 
         win_bucket = round(p_win_winner, 2)) %>%
  filter(!is.na(winner_score)) %>%
  group_by(win_bucket) %>%
  summarise(p = mean(win)) %>%
  ggplot(aes(x = win_bucket, y = p)) + 
  geom_point() + 
  geom_smooth(alpha = 0.7, linetype = "dashed", se = F, method = "loess", formula = "y ~ x") + 
  geom_abline(slope = 1) + 
  annotate("text", x = 0.5, y = 0.49, label = "y = x", hjust = 0) + 
  annotate("text", x = 0.5, y = 0.6, label = "Line of \nbest fit", hjust = 0, color = "blue") + 
  labs(title = "The historical outcome observation lines up well with the projected outcomes", 
       x = "Forecasted win rate", 
       y = "Observed win rate") + 
  theme_bw(12)
```

As shown above, we can see there is a strong correlation between the forecasted 
and observed outcomes. Notably there are outcomes where teams with a ELO forecast 
rate greater than 93% won all of their games, but overall the above scatter plot 
is strong evidence towards using ELO ratings for our projections. 

```{r, message=FALSE}
# Helper Functions ----
read_data = function(path, past_picks, all_weeks = FALSE){
  dt = data.table::fread(path)
  week1 <- as.Date("2021-09-09")
  
  # calculate week, get proj loser and prob of loss. 
  
  dt[, loser:=ifelse(qbelo_prob1 > qbelo_prob2, team2, team1)]
  dt[, winner:=ifelse(qbelo_prob1 > qbelo_prob2, team1, team2)]
  dt[, p_win:=ifelse(qbelo_prob1 > qbelo_prob2, qbelo_prob2, qbelo_prob1)]
  dt[, p_win_winner:=ifelse(qbelo_prob1 > qbelo_prob2, qbelo_prob1, qbelo_prob2)]
  dt[, week:=floor(as.numeric(difftime(date, week1, units="days")) / 7) + 1]
  dt[, loser_score:=ifelse(qbelo_prob1 > qbelo_prob2, score2, score1)]
  dt[, winner_score:=ifelse(qbelo_prob1 > qbelo_prob2, score1, score2)]
  dt = dt[, upset:=ifelse(loser_score > winner_score, TRUE, FALSE)]
  
  dt = dt[, .(week, loser, winner, p_win, p_win_winner, upset)]
  
  return(dt)
}
```

## 2021 NFL Season Data 

To best understand the optimization problem we are facing, lets first look at 
the 2021 NFL season data. There are 18 weeks and 32 teams to pick from in the 
2021 season. From the 
heat map below, we see that Josn Allen and the Buffalo Bills are favourited 
the most by the ELO rankings with Tom Brady and the Buccaneers closeby. There is a 
battle between the Houston Texans and Detroit Lions as the worst teams
in the league. It is also worth noting, Houston has their lowest win rate of the 
season in Week 7, roughly the same rate as Detroit. This will prove to be the
crux across different algorithms. 

```{r}
fivethirtyeight = "https://projects.fivethirtyeight.com/nfl-api/nfl_elo_latest.csv"
plot_data = read_data(fivethirtyeight, all_weeks = TRUE)

plot_data %>% 
  tidyr::pivot_longer(cols = c("loser", "winner"), values_to = "team") %>%
  mutate(p = ifelse(name == "loser", p_win, p_win_winner)) %>%
  select(-c("p_win", "p_win_winner")) %>%
  group_by(team) %>%
  mutate(win_avg = mean(p)) %>%
  ungroup() %>%
  mutate(team = reorder(team, win_avg)) %>% # reorder teams by avg win
  ggplot(aes(week, team, fill=p)) + 
  geom_tile() + 
  scale_fill_viridis_c("Prob. Win") + 
  theme_bw() + 
  labs(title = "Weekly Win Probabilities Heatmap by team", 
       x = "Week Number", 
       y = "Team Name") + 
  scale_x_continuous(expand = c(0, 0))
```

## Determining the optimal pick profile

My approach for the contest was to pick teams based on their opportunity cost of 
not being selected, or in other words, the percentage differential towards the 
next best team to pick in a given week. Additionally, since lasting week to week 
is the most important aspect of the losers pool, I discount the opportunity cost 
so that future weeks are less important than upcoming weeks. 

I also compare the results from my opportunity cost algorithm against a naive 
algorithm that picks the lowest probability team to win each week.

One specific rule in the Losers Pool, is that players are allowed to reenter into 
the pool if they are eliminated in Weeks 1 or 2. For this reason, my optimization 
begins in Week 3, and picks made in Weeks 1 and 2 are the next best teams that are 
remaining for those weeks after the algorithms are completed.

It is also worth noting, losers pools do not last the entire season. Consider 
the case of picking a game where a team has a 15% chance of winning, there is 
roughly a 50% chance of lasting past Week 4, a 20% chance of lasting until Week 10, 
and a mere 5% chance of lasting until Week 18. Therefore, it would be very costly 
to earlier weeks to make pick projections out to the 18th week. Instead, I chose 
to run my algorithms out to Week 10. It also just happens to be that the winner of 
the 2020 NFL Losers pool was determined in Week 10.

In the sections below, I describe three different algorithms to determining the 
set of team-week picks to optimize the likelihood of lasting in the losers pool. 
The three algorithms are: 

1. Taking the best pick per week with no foresight
2. Making picks based on minimizing the opportunity cost of each selected team
3. Minimizing the sum of win probabilities with brute force

### Naive Approach - Picking weekly without foresight
We can use a naive algorithm that has no foresight into future periods to compare 
against the opportunity cost model. Picking the lowest probability team week by 
week will front-load the best teams and will most likely run into sub optimal picks 
in later weeks.

The Lowest Pick per Week algorithm begins in Week 3, picks the team with the 
lowest win probability, then removes that team from being picked in future weeks. 

### Oppertunity Cost Model: 
The benefit of this algorithm comes with its forward-looking nature. The model 
determines each weekly pick based on the delta between the best pick that week 
and the win probability of the next best team. This approach allows for the model 
to pick teams when they are the most valuable, rather than making the best pick 
to move on to the next week. 

The Opportunity Cost algorithm computes the difference between the best and 
second best pick for each week, then picks the team with the largest opportunity 
cost across all weeks in the data set. Then, the selected team and week are discarded 
from the candidate weeks and team, and the algorithm repeats. Additionally, the 
algorithm discounts future weeks by a parameter $\beta = 0.8$ that way more 
weight is placed on upcoming weeks. 

Compared to the naive approach, this model preforms worse in early weeks, however 
out preforms the naive approach the longer the season goes on as the model is 
trading off early performance for sustained performance.

### Brute Force Model

The Brute Force Model strategically works through all possible combinations of 
picking the lowest team per week by changing the order of weeks that are selected. 
Since the algorithms optimize over weeks 3 to 10, there are $8! = 40,320$ possible 
permutations of orders to consider each weekly pick. By running through all the 
possible combinations of week orders to make the lowest team pick, we are able to 
see the cumulative probability of each team losing their game, and then pick the 
team profile that has the greatest probability of lasting all 10 weeks. 

While this model gets at the optimal pick across each week, it struggles with 
scaling as each additional week increases the number of checks is represented 
by $N!$.


```{r, message=FALSE}
# Picking functions ----
join_picks <- function(past_weeks, past_picks){
  picks <- setDT(data.frame(week = past_weeks, loser = past_picks))
  picks <- merge(picks, dt, on = week)
  picks <- picks[, .(week, loser, p_win)]
  
  return(picks)
}

by_oc <- function(dt, past_picks, past_weeks, start_week = 3, total_weeks = 10, beta = 1){
  
  for(i in start_week:total_weeks){
    
    pick <- dt[week %in% c(start_week:total_weeks)]
    pick <- pick[!week %in% past_weeks & !loser %in% past_picks, ]
    pick <- pick[order(week, p_win)]
    pick[, oc:= shift(p_win, 1, type="lead") - p_win, by = week]
    pick[, oc:= oc * beta^(week - start_week)]
    pick <- pick[, .SD[1], week]
    pick <- pick[order(-oc)]
    pick <- pick[, .SD[1]]
    
    past_weeks <- append(past_weeks, pick$week)
    past_picks <- append(past_picks, pick$loser)
    
  }
  
  picks <- join_picks(past_weeks, past_picks)
  
  return(picks)
}

by_week <- function(dt, past_picks, past_weeks, start_week = 3, total_weeks = 10, beta = 1){
  # loop through filling in each week
  for(i in start_week:total_weeks){
    pick <- dt[!loser %in% past_picks & week == i]
    pick[, value:= p_win * beta^(week - start_week)]
    pick <- pick[order(week, value)]
    pick <- pick[, .SD[1]]
    
    past_weeks <- append(past_weeks, pick$week)
    past_picks <- append(past_picks, pick$loser)
    
  }
  
  picks <- join_picks(past_weeks, past_picks)
  
  return(picks)
}

cumprob <- function(picks, inc_weeks = FALSE){
  p <- purrr::accumulate((1-picks$p_win), function(x, y)  x * y)
  weeks <- picks$Week
  if(inc_weeks){ out <- data.frame(p, weeks) } 
  else { out <- data.frame(p) }
  
  out
}

path = "https://projects.fivethirtyeight.com/nfl-api/nfl_elo_latest.csv"
df = read_data(path, all_weeks = TRUE)
df = tidyr::pivot_longer(df, cols = c("loser", "winner"), names_to = c("favourite"), values_to = ("team"))
df = mutate(df, p = case_when(
  favourite == "loser" ~ p_win, 
  favourite == "winner" ~ p_win_winner)) %>%
  select(week, team, p) %>%
  filter(week %in% 1:10)

week_combos = permn(3:8) # 8! or 40320 combinations to consider. 
target = 0

for(order in week_combos){
  week_list = c()
  team_list = c()
  score = 1
  for(i in order){
    tmp = setDT(df)[!team %in% team_list & week == i, ]
    tmp = tmp[order(p), .SD[1]] # take lowest pick per week

    team_list = append(team_list, tmp$team)
    week_list = append(week_list, tmp$week)
    score = score * (1 - tmp$p) 
  }
  if(score > target){
    target = score
    picks = data.frame(week = week_list, team = team_list)
  }
}

other_weeks = data.frame(week = c(1, 2, 9, 10), team = c("CHI", "TEN", "JAX", "CAR"))
brute_search = rbind(picks, other_weeks)
brute_search = merge(brute_search, df, on = .(week, teams)) # add prob back in
```

### Comparing across models

Lets test out the performance of each algorithm. A couple of useful metrics to 
use are the average win likelihood across all weeks - note these statistics 
do not include Weeks 1 and 2 which were not included in the model - as well as 
the cumulative probabilities of reaching a given week. We can see that with this 
set of data, the Brute Force model and Opportunity Cost model are nearly identical 
in terms of their average week by week probabilities. The Brute Force model does 
have a slightly lower standard deviation.

```{r, echo = FALSE, message = FALSE}
dt = read_data("https://projects.fivethirtyeight.com/nfl-api/nfl_elo_latest.csv", past_picks = c())
past_weeks = c(1:2) # skip weeks 1 and 2
past_picks = c("DAL", "TEN") # My first picks after optimizing

pick = by_oc(dt, past_picks, past_weeks, beta = 1)
naive = by_week(dt, past_picks, past_weeks)
names(brute_search) = names(pick)
brute_search = brute_search %>% arrange(week)


avg_pick = mean(pick[3:10, ]$p_win)
avg_naive = mean(naive[3:10, ]$p_win)
avg_ga = mean(brute_search[3:10, ]$p_win)
avg_row = data.frame("Mean", "", avg_pick, "", avg_naive, "", avg_ga)
sd_pick = sd(pick[3:10, ]$p_win)
sd_naive = sd(naive[3:10, ]$p_win)
sd_ga = sd(brute_search[3:10, ]$p_win)
sd_row = data.frame("SD", "", sd_pick, "", sd_naive, "", sd_ga)

tbl = pick %>% 
  left_join(naive, by = "week", suffix = c("_oc", "_week")) %>%
  left_join(brute_search, by = "week")


names(tbl) = c("Week", rep(c("Team", "ProbWin"), 3))
names(avg_row) = names(tbl)
names(sd_row) = names(tbl)

tbl = rbind(tbl, avg_row, sd_row)

kbl(tbl, digits=4, caption = "Optimal Picks per Week") %>%
  kable_classic(full_width=F) %>%
  add_header_above(c(" " = 1, "Oppertunity Cost" = 2, "Lowest Per Week" = 2, "Brute Force" = 2)) %>%
  row_spec(nrow(tbl), bold=T) %>%
  row_spec(nrow(tbl) - 1, bold=T)
```

When looking at the likelihood of reaching a given week, we can see that all three 
models preform pretty similar to each-other, however the Opportunity Cost and 
Brute Force models eventually pass the By Week model. If we stretch the weeks in 
the analysis, I would expect to see even better performance from the Opportunity 
Cost model. 

```{r, message = FALSE}
# Probability of reaching a given week ----
models = list(pick[3:10, ], naive[3:10, ], brute_search[3:10, ])
models = lapply(models, cumprob) %>% 
  bind_cols()
models$week = 3:10
names(models) = c("Oppertunity Cost", "By Week", "GA", "Week")
models %>%
  tidyr::pivot_longer(cols = -Week, names_to = "Model", values_to = "p") %>%
  ggplot(aes(x = Week, y = p, color = Model)) +
  geom_line() + 
  geom_point(alpha = 0.8) + 
  scale_x_continuous(expand = c(0, 0), limits = c(3, 10)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1), labels = percent) + 
  scale_color_viridis_d() + 
  labs(title = "Probability of reaching a given week, for the first 10 weeks",
       x = "Week number", 
       y = "") + 
  theme_bw(12)
```

Lastly, we can look at the differences in week-by-week selections across the 
models. The foresight taken by both the Brute Force and Opportunity Cost models 
come to light in the below plot, specifically in Week 4 where only the Lowest 
per Week model takes Houston, while the Brute Force and Opportunity Cost models 
instead wait until week 6 to take Houston and avoid being stuck with a risky 
Philadelphia.

```{r, message = FALSE}
# Plot choice set of each model ----
pick$approach = "Oppertunity Cost"
naive$approach = "Lowest per Week"
brute_search$approach = "Brute Search"
compare = rbind(pick, brute_search)

ggplot(compare, aes(x = week, y = p_win, color = approach, shape = approach, label = loser)) + 
  geom_line(aes(group = week), color="#e3e2e1", size = 2, alpha = 0.7) + 
  geom_point(size = 3, alpha = 0.7) + 
  geom_text(color = "black", nudge_x = 0.5) + 
  scale_color_viridis_d() + 
  coord_flip() + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 11)) + 
  scale_y_continuous(labels = percent) + 
  labs(title = "Choice set comparison between models", 
       x = "Week", 
       y = "Prob. Win", 
       color = "Model", 
       shape = "Model") + 
  theme_bw(12)
```

## Season Results

Armed with the above insight, I tackled the Losers Pool taking insights from 
both the Opportunity Cost and Brute Search models. Also, a quick disclaimer, 
the game probabilities update after each week, therefore picks in earlier weeks 
in hindsight may look less than optimal as the forecasts of future games have 
changed. With that out of the way, here are the results of the season. 
We began with 33 players, and each choice set is shown below. Eliminations are 
shown by the yellow dots. 

```{r, message=FALSE, warning=FALSE}
ownership <- "https://docs.google.com/spreadsheets/d/1sajv1HXDqzjG2bXwx927MTP_TUKRWsjVSvKAwnaWZcs/edit?usp=sharing"
own <- googlesheets4::read_sheet(ownership) %>%
  tidyr::pivot_longer(cols = contains("Week"), names_to = "Week", values_to = "Pick") %>% 
  mutate(Week = as.numeric(gsub("Week ", "", Week)), 
         OUT = ifelse(Pick == "OUT", TRUE, FALSE), 
         Pick = case_when(Pick == "WFT" ~ "WSH", 
                          Pick == "LVR" ~ "LAR",
                          Pick == "AZ" ~ "ARI",
                          TRUE ~ Pick)) %>%
  group_by(Player, Week) %>%
  left_join(dt, by = c("Week" = "week", "Pick" = "loser")) %>%
  select("Week", "Pick", "p_win", "Player", "OUT", "upset") %>%
  mutate(p_win = case_when(
    is.na(p_win) ~ 0.5, 
    TRUE ~ p_win))
  

ggplot(subset(plot_data, week < 10), aes(x = week, y = p_win)) + 
  geom_point(size = 2, alpha = 0.6, color = "#440154FF") + 
  geom_line(data = subset(own, p_win != 0.5), mapping = aes(x = Week, y = p_win, group = Player), alpha = 0.3) + 
  geom_point(data = subset(own, p_win != 0.5), mapping = aes(x = Week, y = p_win, color = upset), size = 2, alpha = 0.9, stroke = 1) + 
  coord_flip() + 
  xlim(1, 10) + 
  # annotate("text", x = 1 + 0.05, y = 0, label="forecast \nhistorical", hjust = 0, color = "darkgrey") + 
  labs(title = "Choice set path of all contestants, with upsets", 
       y = "Win Probability", 
       x = "Week Number") + 
  theme_bw(12) +
  theme(legend.position="none") + 
  scale_color_manual(values=c("#440154FF", "#FDE725FF")) + 
  scale_y_continuous(labels = percent)


```

The season only lasted to Week 9, with three players splitting the pot. 

Admittedly, I did not follow the above Opportunity Cost Algorithm perfectly. I had 
two entries, and so chose to offset the picks I was making each week to mitigate 
some of the risk. 

```{r, message=FALSE}
# Plot actual selection ----
jordan_picks = own[grepl("Hutch", own$Player), c("Week", "Pick", "p_win", "Player")]
names(jordan_picks) = names(compare)
compare = rbind(compare, jordan_picks)
compare = compare %>% filter(loser != "OUT")

ggplot(compare, aes(x = week, y = p_win, color = approach, shape = approach, label = loser)) + 
  geom_line(aes(group = week), color="#e3e2e1", size = 2, alpha = 0.7) + 
  geom_point(size = 3, alpha = 0.8) + 
  scale_color_viridis_d() + 
  coord_flip() + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 11)) + 
  scale_y_continuous(labels = percent) + 
  labs(title = "Choice set comparison between theoretical model picks, \nand my actual season picks", 
       x = "Week", 
       y = "Prob. Win", 
       color = "Model", 
       shape = "Model") + 
  theme_bw(12)
```

If we compare the cumulative probabilities across the above models, and the actual 
picks I put into practice, we can see that the opportunity cost model ended up with 
the higher probability by Week 10. The only downside is taking on the increased risk 
earlier in weeks 4 and 5. 

```{r, message=FALSE}

# Plot cumprob ----

# project remaining picks for each set of picks
jordan_1 = jordan_picks[jordan_picks$approach == "Hutch 1", ]
jordan_2 = jordan_picks[jordan_picks$approach == "Hutch 2", ]
jordan_1 = by_oc(dt, past_picks = jordan_1$loser[1:8], past_weeks = 1:8, beta = 1, start_week = 1, total_weeks = 10)
jordan_2 = by_oc(dt, past_picks = jordan_2$loser[1:7], past_weeks = 1:7, beta = 1, start_week = 1, total_weeks = 10)

# add to data set
jordan_1$approach = "Jordan 1"
jordan_2$approach = "Jordan 2"
models = list(pick[3:10, ], brute_search[3:10, ], jordan_1[3:10, ], jordan_2[3:10, ])
models = lapply(models, cumprob) %>% 
  bind_cols()
models$week = 3:10
names(models) = c("Oppertunity Cost", "Brute Search", "Jordan 1", "Jordan 2", "Week")
models %>%
  tidyr::pivot_longer(cols = -Week, names_to = "Model", values_to = "p") %>%
  ggplot(aes(x = Week, y = p, color = Model, shape = Model)) +
  geom_line() + 
  geom_point(alpha = 0.8) + 
  scale_x_continuous(expand = c(0, 0), limits = c(3, 10)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1), labels = percent) + 
  scale_color_viridis_d() + 
  labs(title = "Probability of reaching a given week, for the first 10 weeks",
       x = "Week number", 
       y = "") + 
  theme_bw(12)

```

Using the same chart format, what do things look like for all the competitors in the pool?
We don't know the picks that would be made by competitors who've lost in earlier weeks, 
therefore, I will only plot the projected probabilities  up until each person lost.

In order to compare my performance with others, I plotted my pick projections in purple. 
We can see, I was in great shape before losing my second team in Week 7 and top team in 
Week 8. In fact, My probability of making week 8 was greater than that of a majority of 
teams making week 6. 

Despite not making the final 3 in the pool - the payout positions - my algorithm 
placed me in a fantastic position to make money in the pool. 

```{r, message=FALSE}
# plot cumulative probabilities
tbl_data = filter(own, Pick != "OUT" & Week > 2) %>%
  group_by(Player) %>%
  mutate(cum_prob = cumprod(1-p_win), 
         own = ifelse(Player == "Hutch 1" | Player == "Hutch 2", TRUE, FALSE)) %>%
  ungroup() %>%
  group_by(Player) %>%
  mutate(win_avg = max(Week) + (min(cum_prob))) %>%
  ungroup() %>%
  mutate(Player = reorder(Player, win_avg))

ggplot(tbl_data, aes(x = Week, y = cum_prob, group = Player, color = own)) +
  geom_line(alpha = 0.7) + 
  scale_x_continuous(expand = c(0, 0), limits = c(3, 9)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1), labels = percent) + 
  scale_color_viridis_d(direction = -1) + 
  geom_hline(yintercept = min(subset(tbl_data, Player == "Hutch 1")$cum_prob), linetype = "dotted") +
  geom_hline(yintercept = min(subset(tbl_data, Player == "Hutch 2")$cum_prob), linetype = "dotted") +
  labs(title = "Probability of reaching a given week, for all compeditors, \nWith my own picks highlighted",
       x = "Week number", 
       y = "") + 
    theme_bw(12) +
    theme(legend.position = "none")
```

We can show the probabilities of reaching a given week with a heatmap below. The lighter the 
box, the greater the probability of reaching that week. As we can see, I was in fantastic 
shape again to move on, with `Hutch 1` in the lead in Week 8.

```{r, message=FALSE}
# plot heatmap of cum prob
ggplot(tbl_data, aes(y = Player, x = Week, fill = cum_prob)) + 
  geom_tile() + 
  scale_x_continuous(expand = c(0, 0), breaks = 3:9) +
  scale_fill_viridis_c(direction = 1) + 
  labs(title = "Cumulative Probabilities of making a given week, \nordered by the greatest likelihood", 
       x = "Week", 
       y = "", 
       fill = "Pr(W < w)") + 
  theme_bw(12) + 
  theme(panel.grid.major = element_blank())
```


