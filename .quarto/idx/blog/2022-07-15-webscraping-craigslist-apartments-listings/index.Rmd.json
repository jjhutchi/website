{"title":"Webscraping Craigslist Apartments Listings","markdown":{"yaml":{"title":"Webscraping Craigslist Apartments Listings","date":"2022-07-15","slug":[],"categories":["R","Cron","Web Scaping"],"meta_img":"image/image.png","description":"Using R to automatically scan for apartment listings on Craigslist."},"headingText":"Overview","containsRefs":false,"markdown":"\n\n```{r, setup, echo=FALSE}\nknitr::opts_chunk$set(echo = TRUE, \n                      warning = FALSE, \n                      message = FALSE)\n```\n\nWhile looking for an apartment to rent in Toronto, I found myself checking Craigslist multiple times throughout the day for new postings. The recent rising interest rates have led to an increased demand for rental units in Toronto, and consequently made the rental market much more competitive and difficult to maneuver. This post describes the craigslist web crawler bot I created in order to identify new relevant apartment listings, and have the results sent to your phone as push notifications.\n\n<!-- I've recently entered into the rental market while looking for housing  -->\n\n<!-- prior to moving to Toronto. I've found that listings are moving quick,  -->\n\n<!-- and found myself constantly checking for apartment listings on Craigslist.  -->\n\n<!-- This post is used to walk through how I built a Craigslist web scraper in R  -->\n\n<!-- that will send push notifications to my Apple Watch when there is a new posting  -->\n\n<!-- within my apartment constraints.  -->\n\n\nThere are three main sections to this process:\n\n1.  Collect listings from Craigslist using `rvest`\n2.  Send push notifications to phone or watch using `pushoverr`\n3.  Schedule the scraper to run in regular intervals with `cronR`\n\n## 1. Collect listings from Craigslist\n\nThe Craigslist apartment listings URL has a logical structure:\n\n```         \n\"https://toronto.craigslist.org/search/toronto-on/apa?availabilityMode=0\n\\______/\\_______/\\____________________/\\________/\\___________________ ...\n   |        |               |               |                | \n scheme  location         domain         location         arguments  \n```\n\nWe can alter the URL we call to filter listings to satisfy specific requirements. For example, we can use the below arguments to filter the results returned by Craigslist.\n\n| Argument            | Description                                   |\n|---------------------|-----------------------------------------------|\n| `&lat=`, `&lng=`    | Centroid coordinates of search                |\n| `&search_distance=` | Radius of search from centroid                |\n| `&min_price=`       | Minimum listing price                         |\n| `&max_price=`       | Max listing price                             |\n| `&min_bedrooms=`    | Minimum number of bedrooms                    |\n| `&sort=`            | Sorting options, `date` sorts posts by latest |\n\nBelow is an example of how we can generate a URL specific to the filters we are after.\n\n```{r}\n# Setup URL\nbase = \"https://toronto.craigslist.org/search/toronto-on/apa?availabilityMode=0&\"\nLAT = \"43.66532376779693\" # are Rotman school of management\nLNG = \"-79.39860792142129\"\nMAX_PRICE = \"3200\"\nMIN_BDRM = \"2\"\nRADIUS = \"1.2\" # is 1.9Km\nSORT = \"date\"\n\nurl = sprintf(\"%slat=%s&lon=%s&max_price=%s&min_bedrooms=%s&search_distance=%s&sort=%s\", \n              base, \n              LAT, \n              LNG, \n              MAX_PRICE, \n              MIN_BDRM, \n              RADIUS, \n              SORT)\n```\n\nAlternatively, you can use the craigslist website to dial in your search, then copy-paste the URL containing your search query.\n\nWith our URL, let's collect the listings. I suspect the `.result-row`, `.result.page`, and `.result-heading` selectors will line up for searching for apartment listings in other cities.\n\nI am building a data frame containing the post title, link, and time the post was last updated.\n\n```{r}\nlibrary(rvest)\n\npage = read_html(url)\n\ntitles = page |> \n  html_nodes(\".result-heading\") |> \n  html_text(trim = TRUE)\n\nlinks = page |> \n  html_nodes(\".result-row\") |> \n  html_element(\"a\") |> \n  html_attr(\"href\")\n\ntimestamp = page |> \n  html_nodes(\".result-date\") |> \n  html_attr(\"datetime\") \n\ntimestamp = as.POSIXct(timestamp, tz = \"\")\n\n# organize into a data frame\nposts = data.frame(\n  titles, \n  links,\n  timestamp\n)\n```\n\nLet's take a quick look at the data we've extracted from the page.\n\n```{r}\nlibrary(kableExtra)\n\nposts |> \n  head() |>\n  kbl(caption = \"Craigslist apartment listings\") |> \n  kable_paper(\"striped\", full_width = F)\n\n```\n\n## 2. Sending Push Notifications\n\nWhile there are multiple tools available, I had success using the `pushoverr` library paired with the Pushover app. I was able to make an application using Pushover which sends push notifications to my iPhone and Apple Watch.\n\nI found the [blog post by Brian Connelly](https://bconnelly.net/posts/r-phone-home/) to be helpful setting up Pushover.\n\n```{r, eval=FALSE}\n# call in API keys for the Pushoverr app\nsource(\"secrets.R\")\n\npush = function(data) {\n  \n  msg = data$title\n  url = data$link\n  \n  pushoverr::pushover(message = msg, \n                      user = USER_KEY, \n                      app = APP_KEY, \n                      url = url)\n  \n}\n```\n\nNext, we need to add some logic to check the collected posts for new postings. I scheduled the script to run every 20 minutes, and so I can get away with only pushing notifications for listings with a time stamp in the last 30 minutes.\n\n```{r, eval=FALSE}\n# for testing, set time to keep the most recent post\n# time = as.POSIXct(Sys.time(), tz = \"\")\ntime = max(timestamp)\n\nTHRESH = 20\nnotify = posts[difftime(time, timestamp, units = \"mins\") < THRESH,  ]\n\n# send push notifications\nN = nrow(notify)\nfor(i in 1:N) {\n  \n  push(notify[i, ])\n  \n  # sleep 10s between batches of new postings\n  if(i < N) {Sys.sleep(10)}\n}\n\n```\n\n## 3. Scheduling the script\n\nThe magic happens when we can have the script run in the background or while we are away from the computer. `cronR` is the perfect tool for the job here as we can set this script to run in 20 minute intervals. A quick disclaimer, `cronR` runs only on unix/linux, so if you're using Windows you will have to find another task scheduler.\n\n<!-- Optional: Include Wickam here::here() tweet -->\n\n<!-- <blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">The only two things that make <a href=\"https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw\">@JennyBryan</a> ðŸ˜¤ðŸ˜ ðŸ¤¯. Instead use projects + here::here() <a href=\"https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw\">#rstats</a> <a href=\"https://t.co/GwxnHePL4n\">pic.twitter.com/GwxnHePL4n</a></p>&mdash; Hadley Wickham (@hadleywickham) <a href=\"https://twitter.com/hadleywickham/status/940021008764846080?ref_src=twsrc%5Etfw\">December 11, 2017</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> -->\n\n```{r, eval=FALSE}\nlibrary(cronR)\n\nscript = \"scrape.R\" # the path to your script\n\ncmd = cron_rscript(script,\n                   log_append = TRUE,\n                   log_timestamp = TRUE)\n\ncron_add(command = cmd,\n         frequency ='*/20 * * * *', # run every 20 minutes\n         id = \"CL-Toronto-Apt\",\n         description = \"Webscraping Toronto Apartments off Craigslist\",\n         tags = \"webscraping\")\n\n\n```\n\nAnd voila! We've now wrote a script to download the relevant apartment listings, filter for new postings, and send us push notifications for any new listings we may want to check out.\n\nHere are the uninterupted scripts used. You will need to add a `secrets.R` file containing your pushover app API keys. My project directory looks like:\n\n```         \nâ”œâ”€â”€ bot.R\nâ”œâ”€â”€ schedule.R\nâ””â”€â”€ secrets.R\n```\n\n### bot.R\n\n```{r, eval = FALSE}\n\n# prepare Craigslist URL ----\nbase = \"https://toronto.craigslist.org/search/toronto-on/apa?availabilityMode=0&\"\nLAT = \"43.66532376779693\" # are Rotman school of management\nLNG = \"-79.39860792142129\"\nMAX_PRICE = \"3200\"\nMIN_BDRM = \"2\"\nRADIUS = \"1.2\" # is 1.9Km\nSORT = \"date\"\n\nurl = sprintf(\"%slat=%s&lon=%s&max_price=%s&min_bedrooms=%s&search_distance=%s&sort=%s\", \n              base, \n              LAT, \n              LNG, \n              MAX_PRICE, \n              MIN_BDRM, \n              RADIUS, \n              SORT)\n\n\n# collect listings ----\n\npage = read_html(url)\n\ntitles = page |> \n  html_nodes(\".result-heading\") |> \n  html_text(trim = TRUE)\n\nlinks = page |> \n  html_nodes(\".result-row\") |> \n  html_element(\"a\") |> \n  html_attr(\"href\")\n\ntimestamp = page |> \n  html_nodes(\".result-date\") |> \n  html_attr(\"datetime\") \n\ntimestamp = as.POSIXct(timestamp, tz = \"\")\n\n# organize into a data frame\nposts = data.frame(\n  titles, \n  links,\n  timestamp\n)\n\n\n# Send push notifications for new postings ----\n\nsource(\"secrets.R\")\n\npush = function(data) {\n  msg = data$title\n  url = data$link\n  pushoverr::pushover(message = msg, \n                      user = USER_KEY, \n                      app = APP_KEY, \n                      url = url)\n}\n\ntime = as.POSIXct(Sys.time(), tz = \"\")\n\nTHRESH = 20\nnotify = posts[difftime(time, timestamp, units = \"mins\") < THRESH,  ]\n\nN = nrow(notify)\nfor(i in 1:N) {\n  \n  push(notify[i, ])\n  \n  # sleep 10s between batches of new postings\n  if(i < N) {Sys.sleep(10)}\n}\n\n```\n\n### schedule.R\n\n```{r, eval=FALSE}\nlibrary(cronR)\n\nscript = \"scrape.R\" # the path to your script\n\ncmd = cron_rscript(script,\n                   log_append = TRUE,\n                   log_timestamp = TRUE)\n\ncron_add(command = cmd,\n         frequency ='*/20 * * * *', # run every 20 minutes\n         id = \"CL-Toronto-Apt\",\n         description = \"Webscraping Toronto Apartments off Craigslist\",\n         tags = \"webscraping\")\n\n```\n\n### secrets.R\n\n```{r}\nUSER_KEY = \"XXXXXXXXXXXXXXXXXX\"\nAPP_KEY = \"XXXXXXXXXXXXXXXXXX\"\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","theme":{"light":["sandstone","../../_light.scss"],"dark":["sandstone","../../_dark.scss"]},"editor":"source","title":"Webscraping Craigslist Apartments Listings","date":"2022-07-15","slug":[],"categories":["R","Cron","Web Scaping"],"meta_img":"image/image.png","description":"Using R to automatically scan for apartment listings on Craigslist."},"extensions":{"book":{"multiFile":true}}}}}