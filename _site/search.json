[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Jordan Hutchings",
    "section": "",
    "text": "Solving the Hanukkah of Data Puzzles\n\n\n\n\n\n\nR\n\n\nPython\n\n\nCoding Puzzles\n\n\n\nCode solutions for the Hanukkah of Data challenge.\n\n\n\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Strava Activities\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nSpatial Data\n\n\n\nMapping Strava activity data onto contour maps.\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNFL Losers Pool - 2022\n\n\n\n\n\n\nR\n\n\nSports Analytics\n\n\nData Visualization\n\n\n\nOptimizing fantasy picks in an annual NFL losers pool.\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting dynamic maps with mapview\n\n\n\n\n\n\nR\n\n\nSpatial Data\n\n\nData Visualization\n\n\n\nCool mapping features in R with mapview.\n\n\n\n\n\nAug 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping Craigslist Apartments Listings\n\n\n\n\n\n\nR\n\n\nCron\n\n\nWeb Scaping\n\n\n\nUsing R to automatically scan for apartment listings on Craigslist.\n\n\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMasters Fantasy Golf Tournament Dashboard\n\n\n\n\n\n\nSports Analytics\n\n\nCron\n\n\nGoogleSheets\n\n\nWeb Scaping\n\n\n\nBuilding an automated Masters fantasy pool dashboard using R and GoogleSheets.\n\n\n\n\n\nJun 12, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html",
    "href": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html",
    "title": "Webscraping Craigslist Apartments Listings",
    "section": "",
    "text": "While looking for an apartment to rent in Toronto, I found myself checking Craigslist multiple times throughout the day for new postings. The recent rising interest rates have led to an increased demand for rental units in Toronto, and consequently made the rental market much more competitive and difficult to maneuver. This post describes the craigslist web crawler bot I created in order to identify new relevant apartment listings, and have the results sent to your phone as push notifications."
  },
  {
    "objectID": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#overview",
    "href": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#overview",
    "title": "Webscraping Craigslist Apartments Listings",
    "section": "Overview",
    "text": "Overview\nThere are three main sections to this process:\n\nCollect listings from Craigslist using rvest\nSend push notifications to phone or watch using pushoverr\nSchedule the scraper to run in regular intervals with cronR"
  },
  {
    "objectID": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#collect-listings-from-craigslist",
    "href": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#collect-listings-from-craigslist",
    "title": "Webscraping Craigslist Apartments Listings",
    "section": "1. Collect listings from Craigslist",
    "text": "1. Collect listings from Craigslist\nThe Craigslist apartment listings URL has a logical structure:\n\"https://toronto.craigslist.org/search/toronto-on/apa?availabilityMode=0\n\\______/\\_______/\\____________________/\\________/\\___________________ ...\n   |        |               |               |                | \n scheme  location         domain         location         arguments  \nWe can alter the URL we call to filter listings to satisfy specific requirements. For example, we can use the below arguments to filter the results returned by Craigslist.\n\n\n\nArgument\nDescription\n\n\n\n\n&lat=, &lng=\nCentroid coordinates of search\n\n\n&search_distance=\nRadius of search from centroid\n\n\n&min_price=\nMinimum listing price\n\n\n&max_price=\nMax listing price\n\n\n&min_bedrooms=\nMinimum number of bedrooms\n\n\n&sort=\nSorting options, date sorts posts by latest\n\n\n\nBelow is an example of how we can generate a URL specific to the filters we are after.\n\n# Setup URL\nbase = \"https://toronto.craigslist.org/search/toronto-on/apa?availabilityMode=0&\"\nLAT = \"43.66532376779693\" # are Rotman school of management\nLNG = \"-79.39860792142129\"\nMAX_PRICE = \"3200\"\nMIN_BDRM = \"2\"\nRADIUS = \"1.2\" # is 1.9Km\nSORT = \"date\"\n\nurl = sprintf(\"%slat=%s&lon=%s&max_price=%s&min_bedrooms=%s&search_distance=%s&sort=%s\", \n              base, \n              LAT, \n              LNG, \n              MAX_PRICE, \n              MIN_BDRM, \n              RADIUS, \n              SORT)\n\nAlternatively, you can use the craigslist website to dial in your search, then copy-paste the URL containing your search query.\nWith our URL, let’s collect the listings. I suspect the .result-row, .result.page, and .result-heading selectors will line up for searching for apartment listings in other cities.\nI am building a data frame containing the post title, link, and time the post was last updated.\n\nlibrary(rvest)\n\npage = read_html(url)\n\ntitles = page |&gt; \n  html_nodes(\".result-heading\") |&gt; \n  html_text(trim = TRUE)\n\nlinks = page |&gt; \n  html_nodes(\".result-row\") |&gt; \n  html_element(\"a\") |&gt; \n  html_attr(\"href\")\n\ntimestamp = page |&gt; \n  html_nodes(\".result-date\") |&gt; \n  html_attr(\"datetime\") \n\ntimestamp = as.POSIXct(timestamp, tz = \"\")\n\n# organize into a data frame\nposts = data.frame(\n  titles, \n  links,\n  timestamp\n)\n\nLet’s take a quick look at the data we’ve extracted from the page.\n\nlibrary(kableExtra)\n\nposts |&gt; \n  head() |&gt;\n  kbl(caption = \"Craigslist apartment listings\") |&gt; \n  kable_paper(\"striped\", full_width = F)\n\n\nCraigslist apartment listings\n\n\ntitles\nlinks\ntimestamp"
  },
  {
    "objectID": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#sending-push-notifications",
    "href": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#sending-push-notifications",
    "title": "Webscraping Craigslist Apartments Listings",
    "section": "2. Sending Push Notifications",
    "text": "2. Sending Push Notifications\nWhile there are multiple tools available, I had success using the pushoverr library paired with the Pushover app. I was able to make an application using Pushover which sends push notifications to my iPhone and Apple Watch.\nI found the blog post by Brian Connelly to be helpful setting up Pushover.\n\n# call in API keys for the Pushoverr app\nsource(\"secrets.R\")\n\npush = function(data) {\n  \n  msg = data$title\n  url = data$link\n  \n  pushoverr::pushover(message = msg, \n                      user = USER_KEY, \n                      app = APP_KEY, \n                      url = url)\n  \n}\n\nNext, we need to add some logic to check the collected posts for new postings. I scheduled the script to run every 20 minutes, and so I can get away with only pushing notifications for listings with a time stamp in the last 30 minutes.\n\n# for testing, set time to keep the most recent post\n# time = as.POSIXct(Sys.time(), tz = \"\")\ntime = max(timestamp)\n\nTHRESH = 20\nnotify = posts[difftime(time, timestamp, units = \"mins\") &lt; THRESH,  ]\n\n# send push notifications\nN = nrow(notify)\nfor(i in 1:N) {\n  \n  push(notify[i, ])\n  \n  # sleep 10s between batches of new postings\n  if(i &lt; N) {Sys.sleep(10)}\n}"
  },
  {
    "objectID": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#scheduling-the-script",
    "href": "blog/2022-07-15-webscraping-craigslist-apartments-listings/index.html#scheduling-the-script",
    "title": "Webscraping Craigslist Apartments Listings",
    "section": "3. Scheduling the script",
    "text": "3. Scheduling the script\nThe magic happens when we can have the script run in the background or while we are away from the computer. cronR is the perfect tool for the job here as we can set this script to run in 20 minute intervals. A quick disclaimer, cronR runs only on unix/linux, so if you’re using Windows you will have to find another task scheduler.\n\n\n\nlibrary(cronR)\n\nscript = \"scrape.R\" # the path to your script\n\ncmd = cron_rscript(script,\n                   log_append = TRUE,\n                   log_timestamp = TRUE)\n\ncron_add(command = cmd,\n         frequency ='*/20 * * * *', # run every 20 minutes\n         id = \"CL-Toronto-Apt\",\n         description = \"Webscraping Toronto Apartments off Craigslist\",\n         tags = \"webscraping\")\n\nAnd voila! We’ve now wrote a script to download the relevant apartment listings, filter for new postings, and send us push notifications for any new listings we may want to check out.\nHere are the uninterupted scripts used. You will need to add a secrets.R file containing your pushover app API keys. My project directory looks like:\n├── bot.R\n├── schedule.R\n└── secrets.R\n\nbot.R\n\n# prepare Craigslist URL ----\nbase = \"https://toronto.craigslist.org/search/toronto-on/apa?availabilityMode=0&\"\nLAT = \"43.66532376779693\" # are Rotman school of management\nLNG = \"-79.39860792142129\"\nMAX_PRICE = \"3200\"\nMIN_BDRM = \"2\"\nRADIUS = \"1.2\" # is 1.9Km\nSORT = \"date\"\n\nurl = sprintf(\"%slat=%s&lon=%s&max_price=%s&min_bedrooms=%s&search_distance=%s&sort=%s\", \n              base, \n              LAT, \n              LNG, \n              MAX_PRICE, \n              MIN_BDRM, \n              RADIUS, \n              SORT)\n\n\n# collect listings ----\n\npage = read_html(url)\n\ntitles = page |&gt; \n  html_nodes(\".result-heading\") |&gt; \n  html_text(trim = TRUE)\n\nlinks = page |&gt; \n  html_nodes(\".result-row\") |&gt; \n  html_element(\"a\") |&gt; \n  html_attr(\"href\")\n\ntimestamp = page |&gt; \n  html_nodes(\".result-date\") |&gt; \n  html_attr(\"datetime\") \n\ntimestamp = as.POSIXct(timestamp, tz = \"\")\n\n# organize into a data frame\nposts = data.frame(\n  titles, \n  links,\n  timestamp\n)\n\n\n# Send push notifications for new postings ----\n\nsource(\"secrets.R\")\n\npush = function(data) {\n  msg = data$title\n  url = data$link\n  pushoverr::pushover(message = msg, \n                      user = USER_KEY, \n                      app = APP_KEY, \n                      url = url)\n}\n\ntime = as.POSIXct(Sys.time(), tz = \"\")\n\nTHRESH = 20\nnotify = posts[difftime(time, timestamp, units = \"mins\") &lt; THRESH,  ]\n\nN = nrow(notify)\nfor(i in 1:N) {\n  \n  push(notify[i, ])\n  \n  # sleep 10s between batches of new postings\n  if(i &lt; N) {Sys.sleep(10)}\n}\n\n\n\nschedule.R\n\nlibrary(cronR)\n\nscript = \"scrape.R\" # the path to your script\n\ncmd = cron_rscript(script,\n                   log_append = TRUE,\n                   log_timestamp = TRUE)\n\ncron_add(command = cmd,\n         frequency ='*/20 * * * *', # run every 20 minutes\n         id = \"CL-Toronto-Apt\",\n         description = \"Webscraping Toronto Apartments off Craigslist\",\n         tags = \"webscraping\")\n\n\n\nsecrets.R\n\nUSER_KEY = \"XXXXXXXXXXXXXXXXXX\"\nAPP_KEY = \"XXXXXXXXXXXXXXXXXX\""
  },
  {
    "objectID": "blog/2023-08-01-strava-api-viz/index.html",
    "href": "blog/2023-08-01-strava-api-viz/index.html",
    "title": "Visualizing Strava Activities",
    "section": "",
    "text": "I’ve wanted to write a blog post making some data visualization using the Strava API for some time however was never able to come up with a cool idea that was not already common among others blogs.\nFor example, there is the Marcus Volz Strava package which allows you to make some cool visualizations using your Strava data.\nRather than repeat the same usual Strava plots, I’ve focused on mapping some of the hikes I recently went on during a hiking trip in the Rocky Mountains.\nThe goal is to overlay the GPX Strava data with topographic maps of the area to make a set of posters I can print out to commemorate the trip."
  },
  {
    "objectID": "blog/2023-08-01-strava-api-viz/index.html#visualizing-strava-data-collected-from-the-rstrava",
    "href": "blog/2023-08-01-strava-api-viz/index.html#visualizing-strava-data-collected-from-the-rstrava",
    "title": "Visualizing Strava Activities",
    "section": "",
    "text": "I’ve wanted to write a blog post making some data visualization using the Strava API for some time however was never able to come up with a cool idea that was not already common among others blogs.\nFor example, there is the Marcus Volz Strava package which allows you to make some cool visualizations using your Strava data.\nRather than repeat the same usual Strava plots, I’ve focused on mapping some of the hikes I recently went on during a hiking trip in the Rocky Mountains.\nThe goal is to overlay the GPX Strava data with topographic maps of the area to make a set of posters I can print out to commemorate the trip."
  },
  {
    "objectID": "blog/2023-08-01-strava-api-viz/index.html#collecting-the-data-from-strava",
    "href": "blog/2023-08-01-strava-api-viz/index.html#collecting-the-data-from-strava",
    "title": "Visualizing Strava Activities",
    "section": "Collecting the data from Strava",
    "text": "Collecting the data from Strava\nI use the rStrava package to read in the activity data. This package makes the process very straight forward.\nLet’s download the hiking data using rStrava. It is pretty straight forward to link authenticate your account, the process is covered at the rStrava Github in detail.\n\n\nCode\n# packages used\npacman::p_load(rStrava, dplyr, sf, purrr, tmap, raster)\n\n# strava authentication\napp_name = \"XXXXXXXXXX\"\napp_client_id = \"XXXXXXXXXX\"\napp_secret = \"XXXXXXXXXX\"\n\nstoken = httr::config(\n  token = strava_oauth(\n    app_name = app_name,\n    app_client_id = app_client_id,\n    app_secret = app_secret,\n    app_scope = \"activity:read_all\",\n    cache = TRUE\n  )\n)\n\nactivities = stoken |&gt;\n  get_activity_list() |&gt;\n  compile_activities() |&gt;\n  filter(\n    start_date_local &gt;= as.Date(\"2023/07/15\"),\n    start_date_local &lt;= as.Date(\"2023/07/23\")\n  )\n\n\nWith the data, lets make a quick plot of one of the activities. I came across a useful approach to convert the rStrava activity polyline to a simple feature object in this blog by r.iresmi.net.\n\n\nCode\ngp2sf = function(gp) {\n  gp |&gt;\n    googlePolylines::decode() |&gt;\n    map_dfr(function(df) {\n      df |&gt;\n        st_as_sf(coords = c(\"lon\", \"lat\")) |&gt;\n        st_combine() |&gt;\n        st_cast(\"LINESTRING\") |&gt;\n        st_sf()\n    }) |&gt;\n    pull(1)\n}\n\n# plot hike\nhike = activities |&gt;\n  filter(name == \"Edith Cavell Peak\")\n\ngpx = hike |&gt;\n  mutate(geom = gp2sf(map.summary_polyline)) |&gt;\n  st_sf(crs = \"EPSG:4326\")\n\nmap = tm_shape(gpx) +\n  tm_lines()\n\nmap\n\n\n\n\n\nI am plotting the data as a thematic map using the tmap package. I’ve enjoyed using this package as it has many built in features for customizing maps and follows a similar syntax to ggplot.\n\n\nCode\nmap1 = map +\n  tm_style(\"classic\") +\n  tm_layout(title = \"Edith Cavell Peak\", title.position = c(\"center\", \"top\"))\n\n\nmap2 = map +\n  tm_style(\"cobalt\") +\n  tm_layout(\n    main.title = \"Edith Cavell Peak\",\n    main.title.position = c(\"left\")\n  ) +\n  tm_compass(type = \"8star\", position = \"left\", size = 1)\n\nmap3 = map +\n  tm_style(\"bw\") +\n  tm_layout(frame = FALSE) +\n  tm_compass(type = \"arrow\", position = \"left\", size = 1)\n\ntext = sprintf(\"%s\\nHiked on %s\", gpx$name, format(as.Date(gpx$start_date), \"%B %e, %Y\"))\n\nmap4 = map +\n  tm_style(\"albatross\") +\n  tm_credits(text = text, position = c(\"LEFT\", \"BOTTOM\")) +\n  tm_compass(type = \"rose\", position = c(\"right\", \"top\"), size = 2)\n\ntmap_arrange(map1, map2, map3, map4, nrow = 2)"
  },
  {
    "objectID": "blog/2023-08-01-strava-api-viz/index.html#adding-in-the-topographic-lines",
    "href": "blog/2023-08-01-strava-api-viz/index.html#adding-in-the-topographic-lines",
    "title": "Visualizing Strava Activities",
    "section": "Adding in the topographic lines",
    "text": "Adding in the topographic lines\nNow that we have the GPX data in our plot, we need to add the contour lines. This is the trickier portion of the project, and is my motivation for writing these steps into a blog post as it wasn’t obvious first where to get the topographic data from, and then how to overlay the information with the Strava GPX data.\nThe raster package does all of the heavy lifting here. From raster I collect the elevation data using getData(\"SRTM\", lon, lat) which returns\n90m raster data based on the latitude-longitude coordinates provided.\n\n\nCode\nelevation_data = getData(\"SRTM\", lon = gpx$end_latlng2, lat = gpx$end_latlng1)\n\ntm_shape(elevation_data) +\n  tm_raster() +\n  tm_style(\"albatross\") +\n  tm_layout(\n    legend.bg.color = \"white\",\n    legend.text.color = \"black\",\n    legend.title.color = \"black\",\n    legend.position = c(\"right\", \"top\")\n  )\n\n\n\n\n\nNotice the raster data returned covers a lot of land. We will need to crop this down so that we can see the GPX hike data once we add it to the plot. I’m able to crop the raster using the bounding box of the GPX data.\n\n\nCode\nbbox = st_bbox(gpx)\n\nlat = (bbox$ymax + bbox$ymin) / 2\nlng = (bbox$xmax + bbox$xmin) / 2\ndelta = 0.0275\n\nzoomed_data = raster::crop(\n  elevation_data, extent(\n    lng - delta,\n    lng + delta,\n    lat - delta,\n    lat + delta\n  )\n)\n\ntm_shape(zoomed_data) +\n  tm_raster() +\n  tm_style(\"albatross\") +\n  tm_layout(\n    legend.bg.color = \"white\",\n    legend.text.color = \"black\",\n    legend.title.color = \"black\",\n    legend.position = c(\"right\", \"top\")\n  )\n\n\n\n\n\nMoving from the raster data to contour lines is simple with the raster::rasterToCountor() function. The tm_iso function will plot the contour lines and will include the elevation labels.\n\n\nCode\ncontour_lines = rasterToContour(zoomed_data)\n\ntm_shape(contour_lines) +\n  tm_iso()\n\n\n\n\n\nAdding the GPX data to the contour lines is as simple as adding in an additional feature to the plot.\n\n\nCode\ntm_shape(contour_lines) +\n  tm_iso() +\n  tm_shape(gpx) +\n  tm_lines(col = \"grey20\", lwd = 3) +\n  tm_style(\"bw\") +\n  tm_layout(frame.double.line = TRUE)\n\n\n\n\n\nFinally, we can finish this off by adding in some formatting and labeling based on the metadata collected from Strava. Here are some theme options to show off the plot.\n\n\nCode\ninfo_card = sprintf(\n  \"%s\\n%s\\nDistance: %s km\\nElevation: %s m\",\n  format(as.Date(gpx$start_date), \"%B %e, %Y\"),\n  \"Jasper, Alberta\",\n  round(gpx$distance, 2),\n  round(gpx$elev_high - gpx$elev_low, 2)\n)\n\n\ndark = tm_shape(contour_lines) +\n  tm_iso(along.lines = FALSE, size = 0.5, bg.col = \"#3c444d\", col = \"#535f6b\") +\n  tm_shape(gpx) +\n  tm_lines(col = \"#965251\", lwd = 3) +\n  tm_style(\"cobalt\") +\n  tm_credits(\n    text = info_card,\n    bg.color = \"#3c444d\",\n    position = c(\"LEFT\", \"TOP\")\n  ) +\n  tm_layout(\n    frame = TRUE,\n    frame.double.line = TRUE,\n    bg.color = \"#3c444d\",\n    main.title = gpx$name,\n    main.title.position = c(\"center\", \"TOP\"),\n    main.title.color = \"#3c444d\"\n  )\n\nlight = tm_shape(contour_lines) +\n  tm_iso(along.lines = FALSE) +\n  tm_shape(gpx) +\n  tm_lines(lwd = 2) +\n  tm_style(\"classic\") +\n  tm_layout(frame = TRUE, frame.double.line = FALSE) +\n  tm_credits(text = gpx$name, position = c(\"center\", \"TOP\"), width = 1, align = \"center\", bg.color = \"white\", size = 1)\n\ntmap_arrange(dark, light, nrow = 1)\n\n\n\n\n\nCode\n# design 3: landscape fitting hike better\nelevation_data = getData(\"SRTM\", lon = gpx$end_latlng2, lat = gpx$end_latlng1)\nbbox = st_bbox(gpx)\n\nlat = (bbox$ymax + bbox$ymin) / 2\nlng = (bbox$xmax + bbox$xmin) / 2\ndelta = 0.002\n\nzoomed_data = raster::crop(\n  elevation_data, extent(\n    bbox$xmin - delta,\n    bbox$xmax + delta,\n    bbox$ymin - delta,\n    bbox$ymax + delta\n  )\n)\n\ncontour_lines = rasterToContour(zoomed_data)\n\nlandscape = tm_shape(contour_lines) +\n  tm_iso(along.lines = FALSE) +\n  tm_shape(gpx) +\n  tm_lines(col = \"grey20\", lwd = 3) +\n  tm_style(\"classic\")\n\ntmap_save(tm = landscape, filename = \"thumb.png\", width = 4)\nlandscape"
  },
  {
    "objectID": "blog/2023-08-01-strava-api-viz/index.html#arranging-multiple-maps",
    "href": "blog/2023-08-01-strava-api-viz/index.html#arranging-multiple-maps",
    "title": "Visualizing Strava Activities",
    "section": "Arranging multiple maps",
    "text": "Arranging multiple maps\nWe can repeat the process for a list of activities. Then by collecting the maps into a list we can use tmap_arrange() to plot the maps side by side allowing for detailed contours across multiple activity plots.\n\n\nCode\n# Drop Wapta Falls as is more of a walk than a hike\nactivities = activities |&gt;\n  filter(name != \"Wapta Falls\")\n\n# get max bbox across all activities to offset each plot consistently from the center\nbbox_list = lapply(split(activities, activities$name), FUN = function(x) {\n  line_data = x |&gt;\n    mutate(geom = gp2sf(map.summary_polyline)) |&gt;\n    st_sf(crs = \"EPSG:4326\")\n\n  st_bbox(line_data)\n}) |&gt;\n  bind_rows() |&gt;\n  summarise(\n    dx = max(xmax - xmin),\n    dy = max(ymax - ymin)\n  )\n\ndelta_x = bbox_list$dx / 2\ndelta_y = bbox_list$dy / 1.5 # make more vertical\n\nmap_list = lapply(split(activities, activities$name), FUN = function(x) {\n  if (x$name %in% c(\"Wilcox Viewpoint\", \"Edith Cavell Peak\")) {\n    city = \"Jasper, AB\"\n  } else {\n    city = \"Yoho, BC\"\n  }\n\n  # generate hike path\n  line_data = x |&gt;\n    mutate(geom = gp2sf(map.summary_polyline)) |&gt;\n    st_sf(crs = \"EPSG:4326\")\n\n  # get topographic data & set boundary for plot\n  bbox = st_bbox(line_data)\n  lat = (bbox$ymax + bbox$ymin) / 2\n  lng = (bbox$xmax + bbox$xmin) / 2\n  padding = 0.001\n\n  elevation_data = getData(\"SRTM\", lon = lng, lat = lat)\n  zoomed_data = raster::crop(\n    elevation_data, extent(\n      lng - delta_x - padding,\n      lng + delta_x + padding,\n      lat - delta_y - padding - 0.001, # for text in bottom\n      lat + delta_y + padding\n    )\n  )\n  contour_lines = rasterToContour(zoomed_data)\n\n  # make plot\n  out = tm_shape(contour_lines) +\n    tm_iso(along.lines = FALSE) +\n    tm_shape(line_data) +\n    tm_lines(col = \"grey20\", lwd = 2) +\n    tm_style(\"classic\") +\n    tm_layout(frame.double.line = TRUE, main.title = x$name, \n              main.title.size = 1, outer.bg.color = \"grey50\", main.title.color = \"white\",\n              main.title.position = \"center\") \n\n  out\n})\n\nwidth = 7\nheight = width * delta_x / delta_y\n\ntmap_arrange(map_list, nrow = 1)"
  },
  {
    "objectID": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html",
    "href": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html",
    "title": "Masters Fantasy Golf Tournament Dashboard",
    "section": "",
    "text": "This blog posts shows how I use the cronR package to automate a fantasy golf pool with some friends for the golf major tournaments.\nThe process takes four steps:\nLets use the 2022 Masters tournament as an example of how we set up the pipeline."
  },
  {
    "objectID": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#collecting-leaderboard-data",
    "href": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#collecting-leaderboard-data",
    "title": "Masters Fantasy Golf Tournament Dashboard",
    "section": "Collecting Leaderboard Data",
    "text": "Collecting Leaderboard Data\nI’ve found success using two different methods here. Either through rvest to web scrape the leader board data off the web, i.e. ESPN Leaderboard, or through an API. The API route takes more work, but I’ve found the ESPN Developer API to work well during the tournament. Here is the endpoint: https://site.api.espn.com/apis/site/v2/sports/golf/pga/scoreboard.\nFor this demo, we will use the web scraping approach, as there is no guarantee the API will work in the future.\n\n# packages \nlibrary(rvest)\nlibrary(googlesheets4) \nlibrary(cronR)\nlibrary(kableExtra) \nlibrary(dplyr)\nlibrary(tidyr)\n\n\nurl = \"https://www.espn.com/golf/leaderboard?tournamentId=401353232\"\n\ncontent = read_html(url)\nscores = content %&gt;% \n  html_table() %&gt;%\n  bind_rows() %&gt;%\n  select(-1)\n\nscores %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"striped\") %&gt;%\n  scroll_box(height = \"500px\")\n\n\n\n\n\nPOS\nPLAYER\nSCORE\nR1\nR2\nR3\nR4\nTOT\nEARNINGS\nFEDEX PTS\n\n\n\n\n1\nScottie Scheffler\n-10\n69\n67\n71\n71\n278\n$2,700,000\n600\n\n\n2\nRory McIlroy\n-7\n73\n73\n71\n64\n281\n$1,620,000\n330\n\n\nT3\nShane Lowry\n-5\n73\n68\n73\n69\n283\n$870,000\n180\n\n\nT3\nCameron Smith\n-5\n68\n74\n68\n73\n283\n$870,000\n180\n\n\n5\nCollin Morikawa\n-4\n73\n70\n74\n67\n284\n$600,000\n120\n\n\nT6\nCorey Conners\n-3\n70\n73\n72\n70\n285\n$521,250\n105\n\n\nT6\nWill Zalatoris\n-3\n71\n72\n75\n67\n285\n$521,250\n105\n\n\nT8\nSungjae Im\n-1\n67\n74\n71\n75\n287\n$450,000\n91\n\n\nT8\nJustin Thomas\n-1\n76\n67\n72\n72\n287\n$450,000\n91\n\n\nT10\nCharl Schwartzel\nE\n72\n69\n73\n74\n288\n$390,000\n80\n\n\nT10\nCameron Champ\nE\n72\n75\n71\n70\n288\n$390,000\n80\n\n\nT12\nDustin Johnson\n+1\n69\n73\n75\n72\n289\n$330,000\n70\n\n\nT12\nDanny Willett\n+1\n69\n74\n73\n73\n289\n$330,000\n70\n\n\nT14\nKevin Na\n+2\n71\n71\n79\n69\n290\n$225,333\n55\n\n\nT14\nJason Kokrak\n+2\n70\n76\n71\n73\n290\n$225,333\n55\n\n\nT14\nMin Woo Lee\n+2\n73\n75\n72\n70\n290\n$225,333\n0\n\n\nT14\nLee Westwood\n+2\n72\n74\n73\n71\n290\n$225,333\n55\n\n\nT14\nHarry Higgs\n+2\n71\n75\n73\n71\n290\n$225,333\n55\n\n\nT14\nTommy Fleetwood\n+2\n75\n72\n70\n73\n290\n$225,333\n55\n\n\nT14\nHideki Matsuyama\n+2\n72\n69\n77\n72\n290\n$225,333\n55\n\n\nT14\nMatt Fitzpatrick\n+2\n71\n73\n76\n70\n290\n$225,333\n55\n\n\nT14\nTalor Gooch\n+2\n72\n74\n73\n71\n290\n$225,333\n55\n\n\nT23\nJ.J. Spaun\n+3\n74\n70\n75\n72\n291\n$138,000\n41\n\n\nT23\nRobert MacIntyre\n+3\n73\n73\n76\n69\n291\n$138,000\n0\n\n\nT23\nSergio Garcia\n+3\n72\n74\n74\n71\n291\n$138,000\n41\n\n\nT23\nHarold Varner III\n+3\n71\n71\n80\n69\n291\n$138,000\n41\n\n\nT27\nViktor Hovland\n+4\n72\n76\n71\n73\n292\n$111,000\n35\n\n\nT27\nSéamus Power\n+4\n74\n74\n74\n70\n292\n$111,000\n35\n\n\nT27\nJon Rahm\n+4\n74\n72\n77\n69\n292\n$111,000\n35\n\n\nT30\nMarc Leishman\n+5\n73\n75\n71\n74\n293\n$93,150\n28\n\n\nT30\nRussell Henley\n+5\n73\n74\n76\n70\n293\n$93,150\n28\n\n\nT30\nHudson Swafford\n+5\n77\n69\n73\n74\n293\n$93,150\n28\n\n\nT30\nLucas Glover\n+5\n72\n76\n72\n73\n293\n$93,150\n28\n\n\nT30\nSepp Straka\n+5\n74\n72\n76\n71\n293\n$93,150\n28\n\n\nT35\nJoaquin Niemann\n+6\n69\n74\n77\n74\n294\n$75,563\n22\n\n\nT35\nWebb Simpson\n+6\n71\n74\n73\n76\n294\n$75,563\n22\n\n\nT35\nTony Finau\n+6\n71\n75\n74\n74\n294\n$75,563\n22\n\n\nT35\nPatrick Reed\n+6\n74\n73\n73\n74\n294\n$75,563\n22\n\n\nT39\nPatrick Cantlay\n+7\n70\n75\n79\n71\n295\n$63,000\n18\n\n\nT39\nTom Hoge\n+7\n73\n74\n75\n73\n295\n$63,000\n18\n\n\nT39\nSi Woo Kim\n+7\n76\n70\n73\n76\n295\n$63,000\n18\n\n\nT39\nBubba Watson\n+7\n73\n73\n78\n71\n295\n$63,000\n18\n\n\n43\nBilly Horschel\n+8\n74\n73\n79\n70\n296\n$55,500\n15\n\n\nT44\nKevin Kisner\n+9\n75\n70\n75\n77\n297\n$51,000\n13\n\n\nT44\nChristiaan Bezuidenhout\n+9\n73\n71\n77\n76\n297\n$51,000\n13\n\n\n46\nCam Davis\n+12\n75\n73\n79\n73\n300\n$46,500\n12\n\n\n47\nTiger Woods\n+13\n71\n74\n78\n78\n301\n$43,500\n11\n\n\nT48\nAdam Scott\n+14\n74\n74\n80\n74\n302\n$40,050\n10\n\n\nT48\nMax Homa\n+14\n74\n73\n77\n78\n302\n$40,050\n10\n\n\nT50\nMackenzie Hughes\n+15\n73\n75\n77\n78\n303\n$37,350\n9\n\n\nT50\nDaniel Berger\n+15\n71\n75\n77\n80\n303\n$37,350\n9\n\n\n52\nTyrrell Hatton\n+17\n72\n74\n79\n80\n305\n$36,000\n8\n\n\n-\nBrian Harman\nCUT\n74\n75\n--\n--\n149\n--\n0\n\n\n-\nPadraig Harrington\nCUT\n74\n75\n--\n--\n149\n--\n0\n\n\n-\nTakumi Kanaya\nCUT\n75\n74\n--\n--\n149\n--\n0\n\n\n-\nZach Johnson\nCUT\n74\n75\n--\n--\n149\n--\n0\n\n\n-\nK.H. Lee\nCUT\n74\n75\n--\n--\n149\n--\n0\n\n\n-\nSam Burns\nCUT\n75\n74\n--\n--\n149\n--\n0\n\n\n-\nLucas Herbert\nCUT\n74\n76\n--\n--\n150\n--\n0\n\n\n-\nMike Weir\nCUT\n74\n76\n--\n--\n150\n--\n0\n\n\n-\nJordan Spieth\nCUT\n74\n76\n--\n--\n150\n--\n0\n\n\n-\nBrooks Koepka\nCUT\n75\n75\n--\n--\n150\n--\n0\n\n\n-\nRyan Palmer\nCUT\n75\n75\n--\n--\n150\n--\n0\n\n\n-\nXander Schauffele\nCUT\n74\n77\n--\n--\n151\n--\n0\n\n\n-\nKeita Nakajima\nCUT\n72\n79\n--\n--\n151\n--\n0\n\n\n-\nAustin Greaser (a)\nCUT\n74\n77\n--\n--\n151\n--\n0\n\n\n-\nStewart Cink\nCUT\n76\n75\n--\n--\n151\n--\n0\n\n\n-\nAbraham Ancer\nCUT\n72\n79\n--\n--\n151\n--\n0\n\n\n-\nLuke List\nCUT\n77\n75\n--\n--\n152\n--\n0\n\n\n-\nFrancesco Molinari\nCUT\n78\n74\n--\n--\n152\n--\n0\n\n\n-\nBernhard Langer\nCUT\n76\n76\n--\n--\n152\n--\n0\n\n\n-\nGary Woodland\nCUT\n75\n77\n--\n--\n152\n--\n0\n\n\n-\nJustin Rose\nCUT\n76\n76\n--\n--\n152\n--\n0\n\n\n-\nErik van Rooyen\nCUT\n73\n79\n--\n--\n152\n--\n0\n\n\n-\nGuido Migliozzi\nCUT\n75\n77\n--\n--\n152\n--\n0\n\n\n-\nCameron Young\nCUT\n77\n77\n--\n--\n154\n--\n0\n\n\n-\nFred Couples\nCUT\n75\n79\n--\n--\n154\n--\n0\n\n\n-\nGarrick Higgo\nCUT\n72\n83\n--\n--\n155\n--\n0\n\n\n-\nLarry Mize\nCUT\n77\n78\n--\n--\n155\n--\n0\n\n\n-\nJames Piot\nCUT\n81\n74\n--\n--\n155\n--\n0\n\n\n-\nAaron Jarvis (a)\nCUT\n81\n74\n--\n--\n155\n--\n0\n\n\n-\nBryson DeChambeau\nCUT\n76\n80\n--\n--\n156\n--\n0\n\n\n-\nSandy Lyle\nCUT\n82\n76\n--\n--\n158\n--\n0\n\n\n-\nVijay Singh\nCUT\n78\n80\n--\n--\n158\n--\n0\n\n\n-\nMatthew Wolff\nCUT\n81\n78\n--\n--\n159\n--\n0\n\n\n-\nThomas Pieters\nCUT\n79\n80\n--\n--\n159\n--\n0\n\n\n-\nStewart Hagestad (a)\nCUT\n79\n81\n--\n--\n160\n--\n0\n\n\n-\nJosé María Olazábal\nCUT\n77\n84\n--\n--\n161\n--\n0\n\n\n-\nLaird Shepherd\nCUT\n81\n85\n--\n--\n166\n--\n0\n\n\n-\nLouis Oosthuizen\nWD\n76\n--\n--\n--\n76\n--\n0\n\n\n-\nPaul Casey\nWD\n--\n--\n--\n--\n--\n--\n0\n\n\n\n\n\n\n\nThe rules of our pool were to each pick 6 players. We would count the four lowest scores each day, and otherwise assign a score of +8 if you failed to have four players make the cut. The winner is the one with the lowest score for their team at the end of the tournament.\n\nJoin fantasy picks\nWe each fill out our teams by a snake draft often on Zoom. For the 2021 Masters, we filled our our results in a GoogleSheet. We can use the googlesheets4 package to read in data from our GoogleSheet of picks into R.\n\nss = \"https://docs.google.com/spreadsheets/d/14nW_AWYil-jBQ2lC54k_lETGw_O031no-Q3FYvtwsUs/edit#gid=0\"\npicks = read_sheet(ss, sheet = \"draft\")\n\npicks %&gt;%\n  pivot_wider(id_cols = Round, names_from = team, values_from = player) %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"striped\")\n\n\n\n\nRound\nBray\nBret\nCraig\nJacko\nJohn\nJordan\nMike\nRocco\nTrev\n\n\n\n\n1\nJustin Thomas\nRory McIlroy\nCollin Morikawa\nBrooks Koepka\nScottie Scheffler\nCameron Smith\nViktor Hovland\nJon Rahm\nDustin Johnson\n\n\n3\nShane Lowry\nCorey Conners\nPaul Casey\nDaniel Berger\nTommy Fleetwood\nSam Burns\nAbraham Ancer\nBryson DeChambeau\nMatt Fitzpatrick\n\n\n2\nLouis Oosthuizen\nJordan Spieth\nHideki Matsuyama\nWill Zalatoris\nTiger Woods\nJoaquin Niemann\nPatrick Cantlay\nTony Finau\nXander Schauffele\n\n\n4\nTyrrell Hatton\nSungjae Im\nMax Homa\nKevin Kisner\nAdam Scott\nJustin Rose\nPatrick Reed\nBilly Horschel\nRussell Henley\n\n\n5\nBubba Watson\nJason Kokrak\nKevin Na\nCameron Champ\nMarc Leishman\nSergio Garcia\nLee Westwood\nWebb Simpson\nTalor Gooch\n\n\n6\nSeamus Power\nTom Hoge\nThomas Pieters\nMackenzie Hughes\nGary Woodland\nSi Woo Kim\nLucas Herbert\nFrancesco Molinari\nCameron Young\n\n\n\n\n\n\n# join scoreboard data & reshape\nresults = picks %&gt;%\n  select(-Round) %&gt;%\n  left_join(scores %&gt;% select(-c(\"POS\", \"SCORE\", \"TOT\", \"EARNINGS\", \"FEDEX PTS\") ), \n            by = c(\"player\" = \"PLAYER\")) %&gt;%\n  pivot_longer(c(\"R1\", \"R2\", \"R3\", \"R4\"), names_to = \"round\", values_to = \"score\") %&gt;%\n  mutate(score = ifelse(score == \"--\", 80, as.numeric(score))) # set MCs to 80\n\nresults %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"striped\") %&gt;%\n  scroll_box(height = \"500px\")\n\n\n\n\n\nteam\nplayer\nround\nscore\n\n\n\n\nBray\nJustin Thomas\nR1\n76\n\n\nBray\nJustin Thomas\nR2\n67\n\n\nBray\nJustin Thomas\nR3\n72\n\n\nBray\nJustin Thomas\nR4\n72\n\n\nBray\nShane Lowry\nR1\n73\n\n\nBray\nShane Lowry\nR2\n68\n\n\nBray\nShane Lowry\nR3\n73\n\n\nBray\nShane Lowry\nR4\n69\n\n\nBray\nLouis Oosthuizen\nR1\n76\n\n\nBray\nLouis Oosthuizen\nR2\n80\n\n\nBray\nLouis Oosthuizen\nR3\n80\n\n\nBray\nLouis Oosthuizen\nR4\n80\n\n\nBray\nTyrrell Hatton\nR1\n72\n\n\nBray\nTyrrell Hatton\nR2\n74\n\n\nBray\nTyrrell Hatton\nR3\n79\n\n\nBray\nTyrrell Hatton\nR4\n80\n\n\nBray\nBubba Watson\nR1\n73\n\n\nBray\nBubba Watson\nR2\n73\n\n\nBray\nBubba Watson\nR3\n78\n\n\nBray\nBubba Watson\nR4\n71\n\n\nBray\nSeamus Power\nR1\nNA\n\n\nBray\nSeamus Power\nR2\nNA\n\n\nBray\nSeamus Power\nR3\nNA\n\n\nBray\nSeamus Power\nR4\nNA\n\n\nBret\nJordan Spieth\nR1\n74\n\n\nBret\nJordan Spieth\nR2\n76\n\n\nBret\nJordan Spieth\nR3\n80\n\n\nBret\nJordan Spieth\nR4\n80\n\n\nBret\nRory McIlroy\nR1\n73\n\n\nBret\nRory McIlroy\nR2\n73\n\n\nBret\nRory McIlroy\nR3\n71\n\n\nBret\nRory McIlroy\nR4\n64\n\n\nBret\nCorey Conners\nR1\n70\n\n\nBret\nCorey Conners\nR2\n73\n\n\nBret\nCorey Conners\nR3\n72\n\n\nBret\nCorey Conners\nR4\n70\n\n\nBret\nSungjae Im\nR1\n67\n\n\nBret\nSungjae Im\nR2\n74\n\n\nBret\nSungjae Im\nR3\n71\n\n\nBret\nSungjae Im\nR4\n75\n\n\nBret\nTom Hoge\nR1\n73\n\n\nBret\nTom Hoge\nR2\n74\n\n\nBret\nTom Hoge\nR3\n75\n\n\nBret\nTom Hoge\nR4\n73\n\n\nBret\nJason Kokrak\nR1\n70\n\n\nBret\nJason Kokrak\nR2\n76\n\n\nBret\nJason Kokrak\nR3\n71\n\n\nBret\nJason Kokrak\nR4\n73\n\n\nCraig\nCollin Morikawa\nR1\n73\n\n\nCraig\nCollin Morikawa\nR2\n70\n\n\nCraig\nCollin Morikawa\nR3\n74\n\n\nCraig\nCollin Morikawa\nR4\n67\n\n\nCraig\nHideki Matsuyama\nR1\n72\n\n\nCraig\nHideki Matsuyama\nR2\n69\n\n\nCraig\nHideki Matsuyama\nR3\n77\n\n\nCraig\nHideki Matsuyama\nR4\n72\n\n\nCraig\nPaul Casey\nR1\n80\n\n\nCraig\nPaul Casey\nR2\n80\n\n\nCraig\nPaul Casey\nR3\n80\n\n\nCraig\nPaul Casey\nR4\n80\n\n\nCraig\nMax Homa\nR1\n74\n\n\nCraig\nMax Homa\nR2\n73\n\n\nCraig\nMax Homa\nR3\n77\n\n\nCraig\nMax Homa\nR4\n78\n\n\nCraig\nThomas Pieters\nR1\n79\n\n\nCraig\nThomas Pieters\nR2\n80\n\n\nCraig\nThomas Pieters\nR3\n80\n\n\nCraig\nThomas Pieters\nR4\n80\n\n\nCraig\nKevin Na\nR1\n71\n\n\nCraig\nKevin Na\nR2\n71\n\n\nCraig\nKevin Na\nR3\n79\n\n\nCraig\nKevin Na\nR4\n69\n\n\nJacko\nBrooks Koepka\nR1\n75\n\n\nJacko\nBrooks Koepka\nR2\n75\n\n\nJacko\nBrooks Koepka\nR3\n80\n\n\nJacko\nBrooks Koepka\nR4\n80\n\n\nJacko\nWill Zalatoris\nR1\n71\n\n\nJacko\nWill Zalatoris\nR2\n72\n\n\nJacko\nWill Zalatoris\nR3\n75\n\n\nJacko\nWill Zalatoris\nR4\n67\n\n\nJacko\nDaniel Berger\nR1\n71\n\n\nJacko\nDaniel Berger\nR2\n75\n\n\nJacko\nDaniel Berger\nR3\n77\n\n\nJacko\nDaniel Berger\nR4\n80\n\n\nJacko\nKevin Kisner\nR1\n75\n\n\nJacko\nKevin Kisner\nR2\n70\n\n\nJacko\nKevin Kisner\nR3\n75\n\n\nJacko\nKevin Kisner\nR4\n77\n\n\nJacko\nCameron Champ\nR1\n72\n\n\nJacko\nCameron Champ\nR2\n75\n\n\nJacko\nCameron Champ\nR3\n71\n\n\nJacko\nCameron Champ\nR4\n70\n\n\nJacko\nMackenzie Hughes\nR1\n73\n\n\nJacko\nMackenzie Hughes\nR2\n75\n\n\nJacko\nMackenzie Hughes\nR3\n77\n\n\nJacko\nMackenzie Hughes\nR4\n78\n\n\nJohn\nScottie Scheffler\nR1\n69\n\n\nJohn\nScottie Scheffler\nR2\n67\n\n\nJohn\nScottie Scheffler\nR3\n71\n\n\nJohn\nScottie Scheffler\nR4\n71\n\n\nJohn\nTiger Woods\nR1\n71\n\n\nJohn\nTiger Woods\nR2\n74\n\n\nJohn\nTiger Woods\nR3\n78\n\n\nJohn\nTiger Woods\nR4\n78\n\n\nJohn\nAdam Scott\nR1\n74\n\n\nJohn\nAdam Scott\nR2\n74\n\n\nJohn\nAdam Scott\nR3\n80\n\n\nJohn\nAdam Scott\nR4\n74\n\n\nJohn\nTommy Fleetwood\nR1\n75\n\n\nJohn\nTommy Fleetwood\nR2\n72\n\n\nJohn\nTommy Fleetwood\nR3\n70\n\n\nJohn\nTommy Fleetwood\nR4\n73\n\n\nJohn\nMarc Leishman\nR1\n73\n\n\nJohn\nMarc Leishman\nR2\n75\n\n\nJohn\nMarc Leishman\nR3\n71\n\n\nJohn\nMarc Leishman\nR4\n74\n\n\nJohn\nGary Woodland\nR1\n75\n\n\nJohn\nGary Woodland\nR2\n77\n\n\nJohn\nGary Woodland\nR3\n80\n\n\nJohn\nGary Woodland\nR4\n80\n\n\nJordan\nCameron Smith\nR1\n68\n\n\nJordan\nCameron Smith\nR2\n74\n\n\nJordan\nCameron Smith\nR3\n68\n\n\nJordan\nCameron Smith\nR4\n73\n\n\nJordan\nSam Burns\nR1\n75\n\n\nJordan\nSam Burns\nR2\n74\n\n\nJordan\nSam Burns\nR3\n80\n\n\nJordan\nSam Burns\nR4\n80\n\n\nJordan\nJoaquin Niemann\nR1\n69\n\n\nJordan\nJoaquin Niemann\nR2\n74\n\n\nJordan\nJoaquin Niemann\nR3\n77\n\n\nJordan\nJoaquin Niemann\nR4\n74\n\n\nJordan\nSergio Garcia\nR1\n72\n\n\nJordan\nSergio Garcia\nR2\n74\n\n\nJordan\nSergio Garcia\nR3\n74\n\n\nJordan\nSergio Garcia\nR4\n71\n\n\nJordan\nJustin Rose\nR1\n76\n\n\nJordan\nJustin Rose\nR2\n76\n\n\nJordan\nJustin Rose\nR3\n80\n\n\nJordan\nJustin Rose\nR4\n80\n\n\nJordan\nSi Woo Kim\nR1\n76\n\n\nJordan\nSi Woo Kim\nR2\n70\n\n\nJordan\nSi Woo Kim\nR3\n73\n\n\nJordan\nSi Woo Kim\nR4\n76\n\n\nMike\nViktor Hovland\nR1\n72\n\n\nMike\nViktor Hovland\nR2\n76\n\n\nMike\nViktor Hovland\nR3\n71\n\n\nMike\nViktor Hovland\nR4\n73\n\n\nMike\nPatrick Cantlay\nR1\n70\n\n\nMike\nPatrick Cantlay\nR2\n75\n\n\nMike\nPatrick Cantlay\nR3\n79\n\n\nMike\nPatrick Cantlay\nR4\n71\n\n\nMike\nPatrick Reed\nR1\n74\n\n\nMike\nPatrick Reed\nR2\n73\n\n\nMike\nPatrick Reed\nR3\n73\n\n\nMike\nPatrick Reed\nR4\n74\n\n\nMike\nAbraham Ancer\nR1\n72\n\n\nMike\nAbraham Ancer\nR2\n79\n\n\nMike\nAbraham Ancer\nR3\n80\n\n\nMike\nAbraham Ancer\nR4\n80\n\n\nMike\nLee Westwood\nR1\n72\n\n\nMike\nLee Westwood\nR2\n74\n\n\nMike\nLee Westwood\nR3\n73\n\n\nMike\nLee Westwood\nR4\n71\n\n\nMike\nLucas Herbert\nR1\n74\n\n\nMike\nLucas Herbert\nR2\n76\n\n\nMike\nLucas Herbert\nR3\n80\n\n\nMike\nLucas Herbert\nR4\n80\n\n\nRocco\nFrancesco Molinari\nR1\n78\n\n\nRocco\nFrancesco Molinari\nR2\n74\n\n\nRocco\nFrancesco Molinari\nR3\n80\n\n\nRocco\nFrancesco Molinari\nR4\n80\n\n\nRocco\nJon Rahm\nR1\n74\n\n\nRocco\nJon Rahm\nR2\n72\n\n\nRocco\nJon Rahm\nR3\n77\n\n\nRocco\nJon Rahm\nR4\n69\n\n\nRocco\nTony Finau\nR1\n71\n\n\nRocco\nTony Finau\nR2\n75\n\n\nRocco\nTony Finau\nR3\n74\n\n\nRocco\nTony Finau\nR4\n74\n\n\nRocco\nBryson DeChambeau\nR1\n76\n\n\nRocco\nBryson DeChambeau\nR2\n80\n\n\nRocco\nBryson DeChambeau\nR3\n80\n\n\nRocco\nBryson DeChambeau\nR4\n80\n\n\nRocco\nBilly Horschel\nR1\n74\n\n\nRocco\nBilly Horschel\nR2\n73\n\n\nRocco\nBilly Horschel\nR3\n79\n\n\nRocco\nBilly Horschel\nR4\n70\n\n\nRocco\nWebb Simpson\nR1\n71\n\n\nRocco\nWebb Simpson\nR2\n74\n\n\nRocco\nWebb Simpson\nR3\n73\n\n\nRocco\nWebb Simpson\nR4\n76\n\n\nTrev\nDustin Johnson\nR1\n69\n\n\nTrev\nDustin Johnson\nR2\n73\n\n\nTrev\nDustin Johnson\nR3\n75\n\n\nTrev\nDustin Johnson\nR4\n72\n\n\nTrev\nXander Schauffele\nR1\n74\n\n\nTrev\nXander Schauffele\nR2\n77\n\n\nTrev\nXander Schauffele\nR3\n80\n\n\nTrev\nXander Schauffele\nR4\n80\n\n\nTrev\nMatt Fitzpatrick\nR1\n71\n\n\nTrev\nMatt Fitzpatrick\nR2\n73\n\n\nTrev\nMatt Fitzpatrick\nR3\n76\n\n\nTrev\nMatt Fitzpatrick\nR4\n70\n\n\nTrev\nRussell Henley\nR1\n73\n\n\nTrev\nRussell Henley\nR2\n74\n\n\nTrev\nRussell Henley\nR3\n76\n\n\nTrev\nRussell Henley\nR4\n70\n\n\nTrev\nTalor Gooch\nR1\n72\n\n\nTrev\nTalor Gooch\nR2\n74\n\n\nTrev\nTalor Gooch\nR3\n73\n\n\nTrev\nTalor Gooch\nR4\n71\n\n\nTrev\nCameron Young\nR1\n77\n\n\nTrev\nCameron Young\nR2\n77\n\n\nTrev\nCameron Young\nR3\n80\n\n\nTrev\nCameron Young\nR4\n80"
  },
  {
    "objectID": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#prepare-data-compute-sidebets",
    "href": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#prepare-data-compute-sidebets",
    "title": "Masters Fantasy Golf Tournament Dashboard",
    "section": "Prepare Data & Compute Sidebets",
    "text": "Prepare Data & Compute Sidebets\nNow we have the scores read in, lets compute the day-by-day scores for each team. Recall, this is the four lowest scores per team per round.\n\npool_results = results %&gt;%\n  group_by(team, round) %&gt;%\n  arrange(team, round, score) %&gt;%\n  mutate(count_score = ifelse(rank(score, ties.method = \"first\") &lt;= 4, 1, 0)) %&gt;%\n  summarise(value = sum(score * count_score) - 72 * 4) %&gt;%\n  ungroup()\n\ntotal_scores = pool_results %&gt;% \n  pivot_wider(id_cols = team, names_from = round, values_from = value) %&gt;%\n  group_by(team) %&gt;%\n  mutate(Total = sum(across())) %&gt;%\n  arrange(Total)\n\ntotal_scores %&gt;%\n  kbl() %&gt;%\n  kable_paper(\"striped\")\n\n\n\n\nteam\nR1\nR2\nR3\nR4\nTotal\n\n\n\n\nBret\n-8\n6\n-3\n-8\n-13\n\n\nJohn\n-1\n-1\n2\n4\n4\n\n\nJordan\n-4\n4\n4\n6\n10\n\n\nTrev\n-3\n6\n12\n-5\n10\n\n\nCraig\n2\n-5\n19\n-2\n14\n\n\nJacko\n-1\n4\n10\n4\n17\n\n\nMike\n-2\n10\n8\n1\n17\n\n\nRocco\n2\n5\n15\n1\n23\n\n\nBray\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nplayer_scores = results %&gt;%\n  pivot_wider(id_cols = c(\"player\", \"team\"), names_from = round, values_from = score)\n\nplayer_scores %&gt;%\n  select(-team) %&gt;%\n  kbl(col.names = c(\"Team\", \"Round 1\", \"Round 2\", \"Round 3\", \"Round 4\")) %&gt;%\n  pack_rows(index = table(player_scores$team)) %&gt;%\n  kable_paper() %&gt;%\n  scroll_box(height = \"500px\")\n\n\n\n\n\nTeam\nRound 1\nRound 2\nRound 3\nRound 4\n\n\n\n\nBray\n\n\nJustin Thomas\n76\n67\n72\n72\n\n\nShane Lowry\n73\n68\n73\n69\n\n\nLouis Oosthuizen\n76\n80\n80\n80\n\n\nTyrrell Hatton\n72\n74\n79\n80\n\n\nBubba Watson\n73\n73\n78\n71\n\n\nSeamus Power\nNA\nNA\nNA\nNA\n\n\nBret\n\n\nJordan Spieth\n74\n76\n80\n80\n\n\nRory McIlroy\n73\n73\n71\n64\n\n\nCorey Conners\n70\n73\n72\n70\n\n\nSungjae Im\n67\n74\n71\n75\n\n\nTom Hoge\n73\n74\n75\n73\n\n\nJason Kokrak\n70\n76\n71\n73\n\n\nCraig\n\n\nCollin Morikawa\n73\n70\n74\n67\n\n\nHideki Matsuyama\n72\n69\n77\n72\n\n\nPaul Casey\n80\n80\n80\n80\n\n\nMax Homa\n74\n73\n77\n78\n\n\nThomas Pieters\n79\n80\n80\n80\n\n\nKevin Na\n71\n71\n79\n69\n\n\nJacko\n\n\nBrooks Koepka\n75\n75\n80\n80\n\n\nWill Zalatoris\n71\n72\n75\n67\n\n\nDaniel Berger\n71\n75\n77\n80\n\n\nKevin Kisner\n75\n70\n75\n77\n\n\nCameron Champ\n72\n75\n71\n70\n\n\nMackenzie Hughes\n73\n75\n77\n78\n\n\nJohn\n\n\nScottie Scheffler\n69\n67\n71\n71\n\n\nTiger Woods\n71\n74\n78\n78\n\n\nAdam Scott\n74\n74\n80\n74\n\n\nTommy Fleetwood\n75\n72\n70\n73\n\n\nMarc Leishman\n73\n75\n71\n74\n\n\nGary Woodland\n75\n77\n80\n80\n\n\nJordan\n\n\nCameron Smith\n68\n74\n68\n73\n\n\nSam Burns\n75\n74\n80\n80\n\n\nJoaquin Niemann\n69\n74\n77\n74\n\n\nSergio Garcia\n72\n74\n74\n71\n\n\nJustin Rose\n76\n76\n80\n80\n\n\nSi Woo Kim\n76\n70\n73\n76\n\n\nMike\n\n\nViktor Hovland\n72\n76\n71\n73\n\n\nPatrick Cantlay\n70\n75\n79\n71\n\n\nPatrick Reed\n74\n73\n73\n74\n\n\nAbraham Ancer\n72\n79\n80\n80\n\n\nLee Westwood\n72\n74\n73\n71\n\n\nLucas Herbert\n74\n76\n80\n80\n\n\nRocco\n\n\nFrancesco Molinari\n78\n74\n80\n80\n\n\nJon Rahm\n74\n72\n77\n69\n\n\nTony Finau\n71\n75\n74\n74\n\n\nBryson DeChambeau\n76\n80\n80\n80\n\n\nBilly Horschel\n74\n73\n79\n70\n\n\nWebb Simpson\n71\n74\n73\n76\n\n\nTrev\n\n\nDustin Johnson\n69\n73\n75\n72\n\n\nXander Schauffele\n74\n77\n80\n80\n\n\nMatt Fitzpatrick\n71\n73\n76\n70\n\n\nRussell Henley\n73\n74\n76\n70\n\n\nTalor Gooch\n72\n74\n73\n71\n\n\nCameron Young\n77\n77\n80\n80"
  },
  {
    "objectID": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#write-to-googlesheets",
    "href": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#write-to-googlesheets",
    "title": "Masters Fantasy Golf Tournament Dashboard",
    "section": "Write to GoogleSheets",
    "text": "Write to GoogleSheets\nLets write our total table to a GoogleSheet where everyone in the competition can view. The googlesheet can be viewed here.\n\n# add data to sheet\nplayer_scores %&gt;%\n  select(team, player, R1, R2, R3, R4) %&gt;%\n  write_sheet(ss, sheet = \"leaderboard\")\n\n# add a total leader board table\nss %&gt;%\n  range_write(total_scores, sheet = \"leaderboard\", range = \"H1\")\n\n# add date last updated \nss %&gt;%\n  range_write(data = data.frame(Last_updated = as.character(Sys.time())),\n              sheet = \"leaderboard\",\n              range = \"O2\")\n\nFrom here, we can build plots off of the data ranges in our GoogleSheet, and update the formatting however we would like."
  },
  {
    "objectID": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#automate-with-cronr",
    "href": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#automate-with-cronr",
    "title": "Masters Fantasy Golf Tournament Dashboard",
    "section": "Automate with cronR",
    "text": "Automate with cronR\ncronR is a unix/linux tool that allows us to schedule R scripts. We are able to set jobs that will run on specific intervals. For example, we can have our leader board update every 15 minutes during the tournament to provide live updates.\n\nSetting the cron job\nWe use cron_add() to set the cron job. I found when using MacOS that I had to allow cron permissions in system preferences. I found this tutorial to be helpful.\nOnce you install the package, you can also use the RStudio addin, found at Addins &gt; Schedule R scripts on Linux/Unix.\n\nwd = getwd()\nscript = file.path(wd, \"index.rmd\")\ncmd = cron_rscript(script, \n                   log_append = TRUE, \n                   log_timestamp = TRUE)\ncron_add(command = cmd, \n         frequency = \"*/15 * * * *\", \n         id = \"2021-masters-pool\", \n         description = \"update masters pool every 15 mintues\")\n\n\n\nViewing and Removing cron jobs\nThis is as simple as using cron_ls() to list the cron jobs, and cron_rm() to remove jobs by their id.\n\ncron_ls()\n\ncron_rm(\"2021-masters-pool\")"
  },
  {
    "objectID": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#other-comments",
    "href": "blog/2022-06-12-masters-fantasy-golf-tournament-dashboard/index.html#other-comments",
    "title": "Masters Fantasy Golf Tournament Dashboard",
    "section": "Other Comments",
    "text": "Other Comments\nWhile we web scraped the leader board data, I find the API can allow you to get richer data. Specifically hole-by-hole scores. This data allows for other side bets. Specifically having skins or a horse race. The downside is the API changes based on the status of the current event, therefore often requires daily checking.\nAdditionally, you can pull the individual scorecard data from the ESPN player pages. For example, see Rory McIlroy here.\nGoogleSheets also have a read_html() function, which allow you to read the raw data straight into the GoogleSheet. This can remove needing to use R, however you lose the flexibility of being able to update the sheet at whatever period you wish, and you are limited if you wish to do more complex analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jordan Hutchings",
    "section": "",
    "text": "Hello! I am a third year Quantitative Marketing Ph.D. student at the University of Toronto. I hold an MA in Economics from the University of British Columbia.\nMy research revolves around causal inference, industrial organization, and applied microeconomics. I also like to write about sports analytics, data visualization, and using R to simplify everyday challenges.\nFeel free to have a look around!"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Jordan Hutchings",
    "section": "",
    "text": "An experimental investigation into whether choice architecture interventions are considered ethical (With Daniella Turetski, Renante Rondina, Bing Feng, and Dilip Soman), Scientific Reports, 13(1), p. 18334, 2023\n\nReplication Files"
  },
  {
    "objectID": "research.html#papers-in-progress",
    "href": "research.html#papers-in-progress",
    "title": "Jordan Hutchings",
    "section": "Papers in progress",
    "text": "Papers in progress\n\nUnderstanding spontaneous consumer demand - Evidence from docked bikeshare\n\n\nFormally titled: Does bike share benefit local retailers?"
  },
  {
    "objectID": "blog/2022-08-10-plotting-dynamic-maps-with-mapview/index.html",
    "href": "blog/2022-08-10-plotting-dynamic-maps-with-mapview/index.html",
    "title": "Plotting dynamic maps with mapview",
    "section": "",
    "text": "I’ve spent a lot of time working with spatial data recently, and came across a cool tool to display maps that I thought was worth writing about. Here are some of the cool features you can find in mapview.\nI’ll be using the included breweries, and franconia datasets to show off some of the features I found useful for making dynamic maps."
  },
  {
    "objectID": "blog/2022-08-10-plotting-dynamic-maps-with-mapview/index.html#overlaying-points-on-a-polygon",
    "href": "blog/2022-08-10-plotting-dynamic-maps-with-mapview/index.html#overlaying-points-on-a-polygon",
    "title": "Plotting dynamic maps with mapview",
    "section": "Overlaying points on a polygon",
    "text": "Overlaying points on a polygon\nMapview can stack maps on each other with ease. For example, we can take the shape files from the franconia data, and overlay the breweries’ coordinates on the same map. Notice when you click on either a point or polygon you get information about the specific data.\n\npacman::p_load(mapview, leafsync, leaflet.extras2, dplyr)\n\nregions = mapview(franconia,\n  zcol = \"NAME_ASCI\",\n  legend = FALSE\n)\n\nbrew = mapview(breweries,\n  legend = FALSE,\n  cex = breweries$number.of.types, # size based on number of beers\n  col.regions = \"white\") # color points white \n\nregions + brew\n\n\n\n\n\n\n\nComparing two maps side by side\nOne of the cooler features I’ve come across comes from leafsync::sync(), which allows you to place two mapview maps next to each other, and mirror the location of your mouse from one map to the next. Admittedly this isn’t the best usecase for this comparison, but lets compare the maps showing breweries founded before the year 1800 and after.\n\nCUTOFF = 1800\nmap_pre = regions + \n  mapview(breweries |&gt;\n            filter(founded &lt; CUTOFF),\n          legend = FALSE,\n          col.regions = \"blue\")\n\nmap_post = regions + \n  mapview(breweries |&gt;\n            filter(founded &gt;= CUTOFF),\n          legend = FALSE,\n          col.regions = \"green\")\n\nsync(map_pre, map_post)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionally, you can add a slider to compare the two maps in the same map.\n\nmap_pre | map_post\n\n\n\n\n\n\nI’m sure there is plenty more use cases and functionality available within mapview, but these are some of the cases I’ve come across that I’ve found helpful in my workflow."
  },
  {
    "objectID": "blog/2023-12-08-hanukkah-of-data-2023/index.html",
    "href": "blog/2023-12-08-hanukkah-of-data-2023/index.html",
    "title": "Solving the Hanukkah of Data Puzzles",
    "section": "",
    "text": "The Hanukkah of Data provides eight short daily coding puzzles during Hanukkah.  I found this set of puzzles to be a great for improving skills in data wrangling, I initially wrote my solutions in R but these puzzles served as a good practice set for getting some reps in with data cleaning in Python.\nThe puzzle solutions can be a little repetitive, however the solutions are not immediately apparent from the prompts, and I found myself looking forward to solving the riddles each morning. With each puzzle you unlock more of the artwork in Noah’s Rug.\nSpoiler warning – Below are my solutions to the puzzles.\n\n\n\nNoah’s Rug - the beautiful ACSII art made by the dev team\n\n\nLets read in the data, load our packages, and do some pre-processing.\n\nRPython\n\n\n\npacman::p_load(dplyr, stringr, kableExtra)\nload_data = function (f) read.csv(file.path(here::here(\"data\"), f))\n\norders       = load_data(\"noahs-orders.csv\")\norders_items = load_data(\"noahs-orders_items.csv\")\nproducts     = load_data(\"noahs-products.csv\")\ncustomers    = load_data(\"noahs-customers.csv\")\n\n\n\n\nimport pandas as pd\n\norders       = pd.read_csv(\"data/noahs-orders.csv\")\norders_items = pd.read_csv(\"data/noahs-orders_items.csv\")\nproducts     = pd.read_csv(\"data/noahs-products.csv\")\ncustomers    = pd.read_csv(\"data/noahs-customers.csv\")\n\ndata = (\n  customers\n    .merge(orders, on = \"customerid\")\n    .merge(orders_items, on = \"orderid\")\n    .merge(products, on = \"sku\")\n  )\n\n\n\n\n\n\n\n\n\n\nDay 1 - The Investigator\n\n\n\n\n\nSarah brought a cashier over. She said, “Joe here says that one of our customers is a skilled private investigator.”\nJoe nodded, “They showed me their business card, and that’s what it said. Skilled Private Investigator. And their phone number was their last name spelled out. I didn’t know what that meant, but apparently before there were smartphones, people had to remember phone numbers or write them down. If you wanted a phone number that was easy-to-remember, you could get a number that spelled something using the letters printed on the phone buttons: like 2 has “ABC”, and 3 “DEF”, etc. And I guess this person had done that, so if you dialed the numbers corresponding to the letters in their name, it would call their phone number!\n“I thought that was pretty cool. But I don’t remember their name, or anything else about them for that matter. I couldn’t even tell you if they were male or female.”\nSarah said, “This person seems like they are skilled at investigation. I need them to find Noah’s rug before the Hanukkah dinner. I don’t know how to contact them, but apparently they shop here at Noah’s Market.”\nShe nodded at the USB drive in your hand.\n“Can you find this investigator’s phone number?”\n\n\n\n\nRPython\n\n\n\ncustomers |&gt;\n  mutate(name = gsub(\" jr.| iii| v| ii| iv | i\", \"\", tolower(name)),\n         last_name = str_extract(name, \"\\\\w+$\"),\n         name_number = str_replace_all(last_name, \"a|b|c\", \"2\") |&gt; \n            str_replace_all(\"d|e|f\", \"3\") |&gt;\n            str_replace_all(\"g|h|i\", \"4\") |&gt;\n            str_replace_all(\"j|k|l\", \"5\") |&gt;\n            str_replace_all(\"m|n|o\", \"6\") |&gt;\n            str_replace_all(\"p|q|r|s\", \"7\") |&gt;\n            str_replace_all(\"t|u|v\", \"8\") |&gt;\n            str_replace_all(\"w|x|y|z\", \"9\"),\n         phone_number = gsub(\"-\", \"\", phone)) |&gt;\n  filter(name_number == phone_number) |&gt; \n  select(name, phone) \n\n            name        phone\n1 sam tannenbaum 826-636-2286\n\n\n\n\n\nletter_to_phone = {\n  \"a\": \"2\", \"b\": \"2\", \"c\": \"2\",\n  \"d\": \"3\", \"e\": \"3\", \"f\": \"3\",\n  \"g\": \"4\", \"h\": \"4\", \"i\": \"4\",\n  \"j\": \"5\", \"k\": \"5\", \"l\": \"5\",\n  \"m\": \"6\", \"n\": \"6\", \"o\": \"6\",\n  \"p\": \"7\", \"q\": \"7\", \"r\": \"7\", \"s\": \"7\",\n  \"t\": \"8\", \"u\": \"8\", \"v\": \"8\",\n  \"w\": \"9\", \"x\": \"9\", \"y\": \"9\", \"z\": \"9\"\n}\n\nfor index, row in customers.iterrows():\n  name = row['name']\n  phone = row['phone']\n  \n  lname = (\n    name\n    .lower()\n    .replace(\"jr.| iii| v| ii| iv | i\", \"\")\n    .split(\" \")[-1]\n    )\n  \n  for word, replacement in letter_to_phone.items():\n    lname = lname.replace(word, replacement)\n    \n  if str(lname) == str(phone).replace(\"-\", \"\"):\n      print(f\"Solution:\\nName: {name}\\nPhone: {phone}\")\n\nSolution:\nName: Sam Tannenbaum\nPhone: 826-636-2286\n\n\n\n\n\n\n\n\n\n\n\nDay 2 - The Contractor\n\n\n\n\n\nThanks to your help, Sarah called the investigator that afternoon. The investigator went directly to the cleaners to see if they could get any more information about the unclaimed rug.\nWhile they were out, Sarah said, “I tried cleaning the rug myself, but there was this snail on it that always seemed to leave a trail of slime behind it. I spent a few hours cleaning it, and the next day the slime trail was back.”\nWhen the investigator returned, they said, “Apparently, this cleaner had a special projects program, where they outsourced challenging cleaning projects to industrious contractors. As they’re right across the street from Noah’s, they usually talked about the project over coffee and bagels at Noah’s before handing off the item to be cleaned. The contractors would pick up the tab and expense it, along with their cleaning supplies.\n“So this rug was apparently one of those special projects. The claim ticket said ‘2017 JP’. ‘2017’ is the year the item was brought in, and ‘JP’ is the initials of the contractor.\n“But they stopped outsourcing a few years ago, and don’t have contact information for any of these workers anymore.”\nSarah first seemed hopeless, and then glanced at the USB drive you had just put back in her hand. She said, “I know it’s a long shot, but is there any chance you could find their phone number?”\n\n\n\n\nRPython\n\n\n\nbagel_skus = products |&gt; \n  filter(grepl(\"bagel\", tolower(desc))) |&gt; \n  pull(sku)\n\nbagel_order_ids = orders_items |&gt; \n  filter(sku %in% bagel_skus)\n\nbagel_order_customers = orders |&gt; \n  mutate(year = lubridate::year(shipped)) |&gt; \n  filter(orderid %in% bagel_order_ids$orderid, \n         year == 2017) |&gt; \n  pull(customerid)\n\ncustomers |&gt; \n  filter(customerid %in% bagel_order_customers) |&gt; \n  mutate(name = gsub(\" jr.| iii| v| ii| iv | i\", \"\", tolower(name)),\n         last_name = str_extract(name, \"\\\\w+$\"), \n         first_name = str_extract(name, \"^\\\\w+\"), \n         initals = paste0(str_sub(first_name, 1, 1), \n                          str_sub(last_name, 1, 1))) |&gt; \n  filter(initals == \"jp\") |&gt; \n  select(name, phone)\n\n             name        phone\n1 joshua peterson 332-274-4185\n\n\n\n\n\ncust = data[data[\"desc\"].str.contains(\"bagel\", case = False) & \n            data[\"shipped\"].str.contains(\"2017\")]\n\nfor index, row in cust.iterrows():\n  name = row['name']\n  phone = row['phone']\n  \n  names = (\n    name\n      .lower()\n      .replace(\"jr.| iii| v| ii| iv | i\", \"\")\n      .split(\" \")\n    )\n  initals = names[0][0] + names[-1][0]\n  \n  if initals == \"jp\":\n      print(f\"Solution:\\nName: {name}\\nPhone: {phone}\")\n\nSolution:\nName: Joshua Peterson\nPhone: 332-274-4185\n\n\n\n\n\n\n\n\n\n\n\nDay 3 - The Neighbor\n\n\n\n\n\nSarah and the investigator were very impressed with your data skills, as you were able to figure out the phone number of the contractor. They called up the cleaning contractor straight away and asked about the rug.\n“Oh, yeah, I did some special projects for them a few years ago. I remember that rug unfortunately. I managed to clean one section, which revealed a giant spider that startled me whenever I tried to work on it.\n“I already had a fear of spiders before this, but this spider was so realistic that I had a hard time making any more progress. I kept expecting the cleaners would call for the rug, but they never did. I felt so bad about it, I couldn’t face them, and of course they never gave me another project.\n“At last I couldn’t deal with the rug taking up my whole bathtub, so I gave it to this guy who lived in my neighborhood. He said that he was naturally intuitive because he was a Cancer born in the year of the Rabbit, so maybe he was able to clean it.\n“I don’t remember his name. Last time I saw him, he was leaving the subway and carrying a bag from Noah’s. I swore I saw a spider on his hat.”\nCan you find the phone number of the person that the contractor gave the rug to?\n\n\n\n\nRPython\n\n\n\nrabbit_years = c(1939, 1951, 1963, 1975, 1987, 1999)\n\nneighborhood = customers |&gt; \n  filter(phone == \"332-274-4185\") |&gt; \n  pull(\"citystatezip\")\n\ncustomers |&gt; \n  mutate(year = lubridate::year(birthdate), \n         month = lubridate::month(birthdate), \n         day = lubridate::day(birthdate)) |&gt; \n  filter(year %in% rabbit_years, \n         month == 6 & day &gt;= 21 | month == 7 & day &lt;= 22, \n         citystatezip == neighborhood) |&gt; \n  select(name, phone)\n\n           name        phone\n1 Robert Morton 917-288-9635\n\n\n\n\n\nrabbit_years = [1939, 1951, 1963, 1975, 1987, 1999]\nneighborhood = customers[customers[\"phone\"] == \"332-274-4185\"][\"citystatezip\"].iloc[0]\n\ndata[\"birthdate\"] = pd.to_datetime(data[\"birthdate\"])\n\nbirth_month = data[\"birthdate\"].dt.month\nbirth_day = data[\"birthdate\"].dt.day\nbirth_year = data[\"birthdate\"].dt.year\n\njune_f = (birth_month == 6) & (birth_day &gt;= 21)\njuly_f = (birth_month == 7) & (birth_day &lt;= 22)\nrabbit_year_f = birth_year.isin(rabbit_years)\nneighborhood_f = data[\"citystatezip\"] == neighborhood\n\nconditions = (june_f | july_f) & rabbit_year_f & neighborhood_f\n\n# Apply the filter\ndata[conditions][[\"name\", \"phone\"]]\n\n                 name         phone\n198918  Robert Morton  917-288-9635\n\n\n\n\n\n\n\n\n\n\n\nDay 4 - The Early Bird\n\n\n\n\n\nThe investigator called the phone number you found and left a message, and a man soon called back:\n“Wow, that was years ago! It was quite an elegant tapestry.\n“It took a lot of patience, but I did manage to get the dirt out of one section, which uncovered a superb owl. I put it up on my wall, and sometimes at night I swear I could hear the owl hooting.\n“A few weeks later my bike chain broke on the way home, and I needed to get it fixed before work the next day. Thankfully, this woman I met on Tinder came over at 5am with her bike chain repair kit and some pastries from Noah’s. Apparently she liked to get up before dawn and claim the first pastries that came out of the oven.\n“I didn’t have any money or I would’ve paid her for her trouble. She really liked the tapestry, though, so I wound up giving it to her.\n“I don’t remember her name or anything else about her.”\nCan you find the bicycle fixer’s phone number?\n\n\n\n\nRPython\n\n\n\nbakery_skus = products |&gt; \n  filter(grepl(\"bky\", tolower(sku))) |&gt; \n  filter(!grepl(\"bagel\", tolower(desc)))\n\nbakery_order_items = orders_items |&gt; \n  filter(sku %in% bakery_skus$sku, \n         qty &gt; 1) |&gt; \n  pull(\"orderid\")\n\nbakery_orders_customers = orders |&gt; \n  filter(orderid %in% bakery_order_items, \n         lubridate::hour(shipped) &lt; 5,\n         ordered == shipped)\n\ncustomers |&gt; \n  filter(customerid %in% bakery_orders_customers$customerid) |&gt; \n  select(name, phone)\n\n          name        phone\n1 Renee Harmon 607-231-3605\n\n\n\n\n\ndata[\"shipped\"] = pd.to_datetime(data[\"shipped\"])\ndata[\"ordered\"] = pd.to_datetime(data[\"ordered\"])\n\nsku_f = data[\"sku\"].str.contains(\"bky\", case=False)\nhour_f = data[\"shipped\"].dt.hour &lt; 5\nin_store_f = data[\"shipped\"] == data[\"ordered\"]\nqty_f = data[\"qty\"] &gt; 1\n\nconditions = sku_f & hour_f & in_store_f & qty_f\n\ndata[conditions][[\"name\", \"phone\"]].drop_duplicates()\n\n               name         phone\n18500  Renee Harmon  607-231-3605\n\n\n\n\n\n\n\n\n\n\n\nDay 5 - The Cat Lady\n\n\n\n\n\n“Yes, I did have that tapestry for a little bit. I even cleaned a blotchy section that turned out to be a friendly koala.\n“But it was still really dirty, so when I was going through a Marie Kondo phase, I decided it wasn’t sparking joy anymore.\n“I listed it on Freecycle, and a woman in Staten Island came to pick it up. She was wearing a ‘Noah’s Market’ sweatshirt, and it was just covered in cat hair. When I suggested that a clowder of cats might ruin such a fine tapestry, she looked at me funny. She said “I only have ten or eleven cats, and anyway they are getting quite old now, so I doubt they’d care about some old rug.”\n“It took her 20 minutes to stuff the tapestry into some plastic bags she brought because it was raining. I spent the evening cleaning my apartment.”\nWhat’s the phone number of the woman from Freecycle?\n\n\n\n\nRPython\n\n\n\nold_cat_food = products |&gt; \n  filter(grepl(\"senior cat\", tolower(desc))) |&gt; \n  pull(\"sku\")\n\ncat_order_items = orders_items |&gt; \n  filter(sku %in% old_cat_food, \n         qty == 10) |&gt; \n  pull(\"orderid\")\n\norders |&gt; \n  filter(orderid %in% cat_order_items) |&gt; \n  left_join(customers, by = \"customerid\") |&gt; \n  select(name, phone) |&gt; \n  unique()\n\n           name        phone\n1 Nicole Wilson 631-507-6048\n\n\n\n\n\ncat_desc = data[\"desc\"].str.contains(\"senior cat\", case=False)\ncat_qty = data[\"qty\"] == 10\n\ndata[cat_desc & cat_qty][[\"name\", \"phone\"]].drop_duplicates()\n\n               name         phone\n6140  Nicole Wilson  631-507-6048\n\n\n\n\n\n\n\n\n\n\n\nDay 6 - The Bargain Hunter\n\n\n\n\n\n“Why yes, I did have that rug for a little while in my living room! My cats can’t see a thing but they sure chased after the squirrel on it like it was dancing in front of their noses.\n“It was a nice rug and they were surely going to ruin it, so I gave it to my cousin, who was moving into a new place that had wood floors.\n“She refused to buy a new rug for herself–she said they were way too expensive. She’s always been very frugal, and she clips every coupon and shops every sale at Noah’s Market. In fact I like to tease her that Noah actually loses money whenever she comes in the store.\n“I think she’s been taking it too far lately though. Once the subway fare increased, she stopped coming to visit me. And she’s really slow to respond to my texts. I hope she remembers to invite me to the family reunion next year.”\n\n\n\n\nRPython\n\n\n\n# check which prices are offered below wholesale cost \norders_below_cost = orders_items |&gt; \n  left_join(products |&gt; select(wholesale_cost, sku), \n            by = \"sku\") |&gt; \n  mutate(sale = unit_price &lt; wholesale_cost) |&gt; \n  filter(sale) |&gt; \n  pull(orderid)\n\norders |&gt; \n  filter(orderid %in% orders_below_cost) |&gt;\n  group_by(customerid) |&gt; \n  summarise(num_purchases = n()) |&gt;\n  arrange(desc(num_purchases)) |&gt;\n  left_join(customers |&gt; select(customerid, name, phone), by = \"customerid\") |&gt; \n  filter(num_purchases == max(num_purchases)) |&gt; \n  select(name, phone)\n\n# A tibble: 1 × 2\n  name        phone       \n  &lt;chr&gt;       &lt;chr&gt;       \n1 Sherri Long 585-838-9161\n\n\n\n\n\nbelow_cost = data[\"unit_price\"] &lt; data[\"wholesale_cost\"]\n\ncounts = (\n  data[below_cost]\n  .groupby([\"name\", \"phone\"])\n  .agg(n_below_cost = pd.NamedAgg(column=\"orderid\", aggfunc=\"nunique\"))\n)\n\ncounts[counts[\"n_below_cost\"] == counts[\"n_below_cost\"].max()]\n\n                          n_below_cost\nname        phone                     \nSherri Long 585-838-9161            31\n\n\n\n\n\n\n\n\n\n\n\nDay 7 - The Meet Cute\n\n\n\n\n\n“Oh that tapestry, with the colorful toucan on it! I’ll tell you what happened to it.\n“One day, I was at Noah’s Market, and I was just about to leave when someone behind me said ‘Miss! You dropped something!’\n“Well I turned around to see this cute guy holding an item I had bought. He said, ‘I got the same thing!’ We laughed about it and wound up swapping items because I wanted the color he got. We had a moment when our eyes met and my heart stopped for a second. I asked him to get some food with me and we spent the rest of the day together.\n“Before long I moved into his place, but the romance faded quickly, as he wasn’t the prince I imagined. I left abruptly one night, forgetting the tapestry on his wall. But by then, it symbolized our love, and I wanted nothing more to do with it. For all I know, he still has it.”\nCan you figure out her ex-boyfriend’s phone number?\nCan you find her cousin’s phone number?\n\n\n\n\nRPython\n\n\n\nday_6_phone = \"585-838-9161\"\n\npast_customer = customers |&gt; \n  filter(phone == day_6_phone) |&gt; \n  pull(customerid)\n\ncolor_purchases = products |&gt; \n  left_join(orders_items |&gt; select(orderid, sku), by = \"sku\") |&gt;\n  left_join(orders |&gt; select(orderid, customerid, \n                             ordered, shipped), \n            by = \"orderid\") |&gt; \n  filter(grepl(\"col\", tolower(sku)), \n         ordered == shipped) |&gt; \n  mutate(item = str_extract(desc, \"\\\\w+\\\\s\\\\w+\"), \n         colour = str_extract(desc, \"\\\\(\\\\w+\\\\)\"), \n         colour = gsub(\"\\\\(|\\\\)\", \"\", colour))\n\ngf_purchases = color_purchases |&gt; \n  filter(customerid == past_customer, \n         grepl(\"\\\\(\", desc)) \n  \nsearch_for_purchases = function(dat) {\n  \n  gf_item = dat$item\n  gf_colour = dat$colour\n  gf_time = dat$ordered\n  \n  out = color_purchases |&gt; \n    filter(item == gf_item & colour != gf_colour, \n           abs(difftime(ordered, gf_time, units = \"mins\")) &lt; 10)\n  \n  return(out)\n}\n\n# loop through girlfriend's purchases\nlapply(1:nrow(gf_purchases), function(i) {\n  search_for_purchases(gf_purchases[i, ])\n}) |&gt; \n  bind_rows() |&gt; \n  select(customerid) |&gt; \n  left_join(customers, by = \"customerid\") |&gt; \n  select(customerid, name, phone)\n\n  customerid         name        phone\n1       5783 Carlos Myers 838-335-7157\n\n\n\n\n\nday_6_phone = \"585-838-9161\"\n\ndata[\"color\"] = data[\"desc\"].str.extract(r\"\\((\\w+)\\)\")\ndata[\"item\"] = data[\"desc\"].str.extract(r\"(\\w+\\s\\w+)\")\ndata[\"ordered\"] = pd.to_datetime(data[\"ordered\"])\n\ncolor_data = data[data[\"sku\"].str.contains(\"col\", case=False)]\ngf_items = color_data[color_data[\"phone\"] == day_6_phone]\n\nfor color, item, time in zip(gf_items[\"color\"], \n                             gf_items[\"item\"], \n                             gf_items[\"ordered\"]):\n    \n    time_delta = color_data[\"ordered\"] - time\n    diff_color = color_data[\"color\"] != color\n    same_item = color_data[\"item\"] == item\n    same_time = abs(time_delta) &lt; pd.Timedelta(\"10 minutes\")\n    \n    candidate = same_time & same_item & diff_color\n    \n    if candidate.any():\n      color_data[candidate][[\"name\", \"phone\"]].drop_duplicates()\n\n                name         phone\n324740  Carlos Myers  838-335-7157\n\n\n\n\n\n\n\n\n\n\n\nDay 8 - The Collector\n\n\n\n\n\n“Oh that damned woman! She moved in, clogged my bathtub, left her coupons all over the kitchen, and then just vanished one night without leaving so much as a note.\nExcept she did leave behind that nasty carpet. I spent months cleaning one corner, only to discover a snake hiding in the branches! I knew then that she was never coming back, and I had to get it out of my sight.\n“Well, I don’t have any storage here, and it didn’t seem right to sell it, so I gave it to my sister. She wound up getting a newer and more expensive carpet, so she gave it to an acquaintance of hers who collects all sorts of junk. Apparently he owns an entire set of Noah’s collectibles! He probably still has the carpet, even.\n“My sister is away for the holidays, but I can have her call you in a few weeks.”\nThe family dinner is tonight! Can you find the collector’s phone number in time?\n\n\n\n\nRPython\n\n\n\n# find customer with the most collectible (col) purchases\nproducts |&gt; \n  filter(grepl(\"col\", tolower(sku))) |&gt; \n  left_join(orders_items |&gt; select(orderid, sku), by = \"sku\") |&gt;\n  left_join(orders |&gt; select(orderid, customerid), by = \"orderid\") |&gt; \n  group_by(customerid) |&gt; \n  tally() |&gt; \n  left_join(customers |&gt; select(customerid, name, phone), by = \"customerid\") |&gt;\n  arrange(desc(n)) |&gt; \n  filter(n == max(n)) |&gt; \n  select(name, phone)\n\n# A tibble: 1 × 2\n  name        phone       \n  &lt;chr&gt;       &lt;chr&gt;       \n1 James Smith 212-547-3518\n\n\n\n\n\ncollectables = data[data[\"sku\"].str.contains(\"col\", case=False)]\n\n(\n    collectables\n    .groupby([\"name\", \"phone\"])\n    .agg(n_collectables=pd.NamedAgg(column=\"orderid\", aggfunc=\"nunique\"))\n    .sort_values(\"n_collectables\", ascending=False)\n    .head(1)\n)\n\n                          n_collectables\nname        phone                       \nJames Smith 212-547-3518             109"
  },
  {
    "objectID": "blog/2022-09-07-nfl-losers-pool-2022/index.html",
    "href": "blog/2022-09-07-nfl-losers-pool-2022/index.html",
    "title": "NFL Losers Pool - 2022",
    "section": "",
    "text": "Coming out of of Labour Day weekend only means one thing, it is time again for the annual NFL Losers Pool competition. This is the second time I am writing about this type of competition. You can see my blog post about trying to draft an optimal lineup here: 2021 NFL Losers Pool.\nBelow are the rules for our 2022 contest."
  },
  {
    "objectID": "blog/2022-09-07-nfl-losers-pool-2022/index.html#losers-pool-rules",
    "href": "blog/2022-09-07-nfl-losers-pool-2022/index.html#losers-pool-rules",
    "title": "NFL Losers Pool - 2022",
    "section": "Losers Pool Rules",
    "text": "Losers Pool Rules\n\nYou must pick exactly one team per week to lose their game.\nYou cannot pick the same team more than once per season.\nIf your team wins their game, you are eliminated.\nRebuys back into the competition are allowed for Weeks 1 and 2.\nYou may enter up to three sets of picks."
  },
  {
    "objectID": "blog/2022-09-07-nfl-losers-pool-2022/index.html#pick-optimization",
    "href": "blog/2022-09-07-nfl-losers-pool-2022/index.html#pick-optimization",
    "title": "NFL Losers Pool - 2022",
    "section": "Pick Optimization",
    "text": "Pick Optimization\nThe objective of this competition is to outlast the other competitors in the pool. Specifically, this means avoiding elimination and being the remaining player in the pool. The second point is worth noting because we will shift our strategy from simply minimizing the risk of our picks losing, to maximizing the likelihood that our picks move on relative to the picks of others in the pool. A quick foreshadowing - this will involve using team ownership to trade-off probability of making it to the next week for increasing our expected value in the competition.\nThere are a total of 32 teams to choose from, and we can expect the pool to run for roughly 10 weeks - going off of last years competition. This is a large number of potential combinations of teams to select in each week. In fact for 10 weeks, it is \\(32 \\times 31 \\times ... \\times 22\\) which is roughly 234 trillion combinations (I’m not including teams with bye weeks but you get the idea, the space of possible picks is very large).\nFortunately, we can be smart about our optimization, and conditional on game forecasts, reach the global optimum without much computation work. I use two different algorithms to compare pick schedules; what I call the Opportunity Cost Model and Greedy Model. The Greedy Model will out preform the Opportunity Cost model in the short run, but eventually the Opportunity Cost model will pass the Greedy Model in future weeks.\n\nOpportunity Cost Model - picking the lowest win probability team in a given week conditional on it having the largest distance to the second lowest win probability that same week.\nGreedy Model - Picking the team with the lowest win probability in the first week, then the second, and so on…\n\nOpportunity Cost Model Algorithm\n\nStep 1: Compute the difference between the least and second least likely teams to win in each week for each team and week in the pool.\nStep 2: Pick the team & week combination with the largest difference between the least and second least likely teams.\nStep 3: Remove the week and team combination from the pool and repeat Steps 1 & 2 until all weeks are filled.\n\n  \nGreedy Model Algorithm\n\nStep 1: Start at the earliest week we wish to optimize over.\nStep 2: Pick the team with the lowest probability of winning, and remove this team from the candidate pool.\nStep 3: Move on to the next week, and repeat Steps 2 and 3 until we reach the terminal week."
  },
  {
    "objectID": "blog/2022-09-07-nfl-losers-pool-2022/index.html#making-picks",
    "href": "blog/2022-09-07-nfl-losers-pool-2022/index.html#making-picks",
    "title": "NFL Losers Pool - 2022",
    "section": "Making Picks",
    "text": "Making Picks\nLets put the above algorithms to action. Like last year, I am using the FiveThirtyEight NFL Projections to estimate each teams likelihood of winning their game. These ratings are based off of each teams computed ELO score, with some additional adjustments - read about their methodology here.\nWe can see that there are some clear weeks below with drastic underdogs, and each week after Week 1 contains at least one game with a win probability less than 25%.\n\n\n\n\n\n\n\n\n\nI choose to run the above two algorithms starting in Week 3. Since we can rebuy back into the competition in Weeks 1 and 2, we do not want to take a valuable pick from our elimination weeks. Therefore, I make my set of picks on weeks 3 through 10, then pick Week 1 and 2 after removing the Weeks 3 - 10 picks, this ended up being the Pittsburgh Steelers and Chicago Bears.\nThe pick schedules using both algorithms are shown below. Notice the trade off of early week win probabilities for later risk savings.\n\n\n\n\n\n\n\n\n\nWe can compare the performance of both algorithms by comparing the likelihoods of reaching a given week for both models. The likelihood we move on from a given week \\(w\\) is equal to the probability \\(P(W\\leq w)\\) where,\n\\[\\begin{align*}\nP(W\\leq w) &= \\Pi_{w=3}^{12} p_{i, w}\\cdot x_{i, w} \\\\\n\\text{Subject to } & \\sum_i x_{i, w} = 1 \\\\\n& \\sum_w x_{i, w} \\leq 1\n\\end{align*}\\]\nWhich is the likelihood a given schedule of picks reaches week 10 subject to being able to pick only one team per week, and picking any given team at most once."
  },
  {
    "objectID": "blog/2022-09-07-nfl-losers-pool-2022/index.html#optimal-decisions-under-multiple-entries",
    "href": "blog/2022-09-07-nfl-losers-pool-2022/index.html#optimal-decisions-under-multiple-entries",
    "title": "NFL Losers Pool - 2022",
    "section": "Optimal Decisions under Multiple Entries",
    "text": "Optimal Decisions under Multiple Entries\nOne interesting aspect of the Losers Pool is that we are able to submit multiple submissions into the competition. The above analysis works for the Single-Entry case, however things become more complex in the Multiple-Entry case. When dealing with multiple entries, I move from minimizing the cost of being eliminated in any given week, to minimizing the likelihood all of the entries are eliminated in a given week. We can represent each combination of an N-team tuple as a possible pick in a given week, and calculate the probability of at least one team moving onto the next week from a tuple of N picks. Given we have two entries in the competition, this is equal to \\(1 - (p_{i, w} \\cdot x_{i, w})(p_{j, w} \\cdot x_{j, w})\\).\nBy computing the above likelihoods of each tuple moving onto the following week, we can then run the Opportunity Cost model on the set of picks, taking the pick with the largest difference between the best and second best pick across all weeks we are considering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above plot has its axis removed, however is interactive. By hovering each cell you can see the pick tuple as well as the likelihood one of the two teams moves on. The darker the cell, the greater the likelihood of at least one team losing their game that week.\nSome observations:\n\nIt is almost always best to diversify your picks. Despite Atlanta having a low win rate in Week 5, combinations of picking Atlanta and another team still dominate picking Atlanta.\nThe matrix is symmetric, however ordering of picks matters as picking a team in the first spot can still allow for the same team to be picked in the second spot. i.e. (DET, JAX) and (JAX, DET) are valid in consecutive weeks.\n\nAs above, we can compare the performance of the OC model with the Greedy Model in the Multi-Entry case. The below plot is in terms of the likelihood of moving onto the following week, and so a higher is better. As with the Single-Entry case, we can see the OC model trading off some early likelihood for greater future likelihoods of having at least one team move onto the next week."
  },
  {
    "objectID": "blog/2022-09-07-nfl-losers-pool-2022/index.html#results",
    "href": "blog/2022-09-07-nfl-losers-pool-2022/index.html#results",
    "title": "NFL Losers Pool - 2022",
    "section": "Results",
    "text": "Results\nBelow are picks per entry, where picks which won their game are shown with red text. Once a player is removed, they no longer show up on the plot, and players are sorted in ascending order in terms of their average win probability - players higher in the plot have had lower win probabilities up to the shown week.\n\n\n\n\n\n\n\n\n\nI followed the Opportunity Cost model recommendations for most of my picks. I swapped out the recommended New England in Week 4 for the New York Jets as none of the remaining players in the pool had yet to choose NE, and it is beneficial to separate yourself from the group given how often we see upsets. Unfortunately for me, it was the NYJ and not NE game that resulted in an upset.\nAnd just like that, that is a wrap for the 2022 Losers Pool! Big congrats to Drew2, Billy and DM who chose to split the pot following Week 7. Hopefully we can improve on our algorithms and selection process come the 2023 season. I think there is some value to be found in optimizing across multiple entries, as well as being able to shift from minimizing the risk of being eliminated to maximizing the expected value of a set of picks conditional on who the remaining players have left in the pool."
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Miscellaneous Information",
    "section": "",
    "text": "Here is a catch all for links and resources I find useful.\n\nTextbooks\n\nR for Data Science\nMastering R Shiny\nBuilding reproducible analytical pipelines with R\nCausal Inference The Mixtape\n\n\n\nBlog posts\n\nOne Year to Dissertate: Writing your dissertation in RStudio.\nYihui Xie | One Markdown Document, Fourteen Demos: Showing off some of the capabilities available in R Markdown.\nSusan Athey on Ph.D. Applications"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Jordan Hutchings",
    "section": "",
    "text": "An experimental investigation into whether choice architecture interventions are considered ethical (With Daniella Turetski, Renante Rondina, Bing Feng, and Dilip Soman), Scientific Reports, 13(1), p. 18334, 2023\n\nReplication Files"
  },
  {
    "objectID": "research.html#past-work",
    "href": "research.html#past-work",
    "title": "Jordan Hutchings",
    "section": "Past work",
    "text": "Past work\n\n\n\nThe impact of easing stay at home orders on US businesses during the COVID-19 pandemic\n\nM.A. Economics Summer Paper, draft\n\nDoes Yelp impact where we choose to eat?\n\nUndergraduate Thesis, draft, poster"
  },
  {
    "objectID": "research.html#papers-ive-contributed-to",
    "href": "research.html#papers-ive-contributed-to",
    "title": "Jordan Hutchings",
    "section": "Papers I’ve contributed to",
    "text": "Papers I’ve contributed to\n\nDo Virtue Signals Signal Virtue? (Matt Lowe, Deivis Angeli, and the Village Team)  \nFriends with Health Benefits: A Field Experiment (Rachel Gershon, Cynthia Cryder, and Katherine L. Milkman)"
  }
]